<p>This directory aims to combine the current working chapters into one manuscript for previewing the current state of the conversion.</p>
<p>March 18, 2019</p>
<p>Stefan Bender, Hendrik Doll, Christian Hirsch</p>
<h1 id="wheres-waldo-the-search-for-data-in-empirical-research">Where’s Waldo: The search for data in empirical research</h1>
<h2 id="chapter-whos-waldo-conceptual-issues-when-characterizing-data-in-empirical-research">Chapter: Who’s Waldo: Conceptual issues when characterizing data in empirical research</h2>
<h3 id="abstract">Abstract</h3>
<p>Empirical economic and social science research uses microdata for analyses to connect theory to societal problems. At present, discussion centers on the impact of this research – for example as evidence based policy evaluation or analysis –, while little is known about the added value of the used (micro) data. Research data is often considered a public good and much data is generated by publically funded researchers or is available from official statistics (like administrative data). In turn, microdata should help to create new insights of challenges for society.</p>
<p>We present conceptual lessons learned from a machine learning competition held to automate the discovery of datasets, research methods and fields in research publications. Put differently, before you ask where Waldo is, you have to identify, who Waldo is. The obtained information can be used to build up a user-centric dataset recommendation and dataset impact evaluation system. Our main conclusion is that the research community is able to build such a system. There is a strong need to establish a data sharing culture, encouraged by giving credit where credit is due through dataset citations.</p>
<h3 id="table-of-contents">Table of contents</h3>
<ol type="1">
<li>Introduction</li>
<li>Insights from a research data centre perspective</li>
<li>Conceptual lessons learned</li>
<li>Conclusions</li>
<li>References</li>
</ol>
<h3 id="introduction">1. Introduction</h3>
<p>Policy makers increasingly recognize that informed decision-making – for example – in the field of monetary policy requires microdata on financial institutions and markets to uncover interdependencies between entities and document global developments. Making microdata available entails costs which are related to the need to safeguarding statistical confidentiality and making accompanying metadata available.</p>
<p>At Deutsche Bundesbank (the German central bank), the Research Data and Service Centre (RDSC) offers access to microdata free of charge for independent research. As one of the largest data producers in Germany, Bundesbank offers access to its high quality micro data for research purposes. Since access to microdata is subject to legal and data protection requirements, Bundesbank set up the RDSC, which provides researchers and analysts with access to selected micro data in the context of independ-ent scientific research projects.</p>
<p>To strengthen effective quantitative research through optimal microdata usage, the RDSC has engaged in a series of projects that are targeted at i) quantifying the marginal benefit of microdata provision for society in general and Deutsche Bundesbank in particular, and ii) enhancing user experience. The competition at hand is a first, but major step towards producing a framework for a user-centric dataset recommendation system and dataset impact evaluation. As such, the competition is intended to showcase the feasibility of such a comprehensive approach.</p>
<p>One specific project currently pursued by the RDSC is the development of a microdata map, which will serve as an important input for a data-driven recommendation system for researchers. Traditionally, metadata on microdata is provided from a producer’s perspective (e.g. data collection methods, universe, sampling design, regulatory requirements, etc.). The envisioned, user-centric, data driven recommendation system, is novel in terms of providing information about how other researchers have actually used microdata in previous research.</p>
<p>While an important part of the dataset discovery process and for informing researchers, traditionally compiled metadata is insufficient to recommend microdata to researchers in the finance domain for two reasons. First, the discovery of newly compiled microdata may be slow because these datasets typically do not have annotated expert knowledge, when they first become available for research. Second, while microdata might seem similar from the data producer side, researchers may venture to link these datasets for empirical research.</p>
<p>Therefore, a challenge in building a data-driven recommendation system is to make sure that recommended datasets are indeed feasible to use, i.e. constitute meaningful recommendations. Thus, besides information about datasets, additional information such as fields and methods is needed to be ingested into the system. This additional information essentially constitutes additional links between datasets that helps better align datasets. This is especially true in the finance domain where linking microdata is a common feature in empirical research.</p>
<p>Empirical research papers are a natural source of information about dataset use. However, as any researcher or librarian may confirm, hand-curating a large corpus of research papers is labor-intensive and error-prone. Therefore, being able to automatically retrieve the necessary information from research papers lays the groundwork for any future dataset recommendation system. The competition is an important first step and proofs that data set extraction is shown to be manageable and scalable.</p>
<p>Extracting dataset citations from publications is a fairly difficult task because of the variety of dataset citation formats and the absence of training data (for a recent overview see Koesten et al., 2019). Boland et al (2012) propose a weakly supervised approach, using a pattern induction method for the detection of study references in full texts. They use a corpus of 259 publications from the Social Science Open Access Repository (SSOAR). They use a bootstrapping approach, starting with a small corpus of manually created training instances. The resulting system InfoLink now informs SSOAR.</p>
<p>As Boland and Mathiak (2015) describe dataset extraction as a twofold task, finding dataset citation string and following entity resolution (match the string to the correct entity/ DOI). Concerning entity resolution, they report the difficulty of broad survey dataset citations that ignore data variability (such as years, versions, questionnaire variants, etc.), motivating a dataset taxonomy. Named dataset citations are often underspecified allowing identification of the survey but not of the precise dataset (which of mul-tiple subsamples, aggregation levels, survey modes, etc.).</p>
<p>Zhang et al (2016) also use a bootstrapping approach to extract dataset citations from 116 computer science journals publications. Ghavimi et al. (2016) use a similar approach for social science papers finding datasets with well-documented metadata. According to them, only 25% of all dataset citations are given in the references, highlighting the unstructured citation culture for datasets. We advance from these with an environment with less available dataset metadata and a corpus of publications from a variety of fields for our purposes. To tackle this, we continue with a larger hand-curated annotated corpus.</p>
<p>Metadata schemas for datasets are available, such as DataCite metadata schema and the da|ra metadata schema, which complies with the DataCite schema (Helbig et al. 2014). They offer dataset taxonomies and standardized citation propositions, however their categories do not optimally support automatic search and extraction.</p>
<p>Improving dataset citation is high on the scientific agenda in recent years. This notably includes promoting widespread usage of persistent and unique dataset identifiers. As available datasets are spread across a large number of databases, identification of datasets is important for reproducibility and to credit data creation efforts to incentivize data creation and publication (Lagoze and Vilhuber 2017, McMurry et al. 2017, Mooney and Newton 2012). If unique and persistent dataset identification in publications were available, Ball and Duke (2011) raise the idea of dataset impact factors with such information.</p>
<p>We present lessons learned from the machine learning competition held to automate the discovery of datasets, associated research methods and fields in social science research publications. In doing so, we show our insights about dataset taxonomies from our experience in a research data centre and from designing a machine learning competition. We do this with a background of all authors in social science.</p>
<p>The outcome of the competition contributes to the ongoing digitalization efforts of the Deutsche Bundesbank. Extracting relevant information from research papers as an unstructured data source broadens the value of unstructured, underexplored, data. Thus, the project presents a well-defined use-case to turn tacit knowledge into codified knowledge by converting text into relatively well-structured information. The results can become a blueprint for future projects, where information is less structured.</p>
<h3 id="insights-from-a-research-data-centre-perspective">2. Insights from a research data centre perspective</h3>
<h4 id="background">2.1 Background</h4>
<p>In response to the increased demand for microdata, in 2013 the Bundesbank set up the Integrated Microdata-based Information and Analysis System (IMIDIAS). One component of IMIDIAS is the recently established Research Data and Service Centre (RDSC). The RDSC applies a standardised procedure to generate high-quality datasets that cover a large part of data requests for research purposes. Thereby the RDSC grants internal and external researchers access − subject to clear conditions − to selected microdata provided by Deutsche Bundesbank, and serves as an interface between data producers and data users.</p>
<p>Requests to use microdata are first reviewed pursuant to legal requirements. The RDSC provides anonymized datasets on banks, securities, investment funds, enterprises and households, all of which can be accessed at dedicated researcher workstations or for most of the Bundesbank’s surveys – as for the Panel on Household Finances (PHF) study – the RDSC offers so called scientific use files.</p>
<p>In addition, the RDSC provides advice on data selection, data content and analytical approaches. Together with the relevant statistical experts, it ensures that the microdata provided are documented in detail and archived. In doing so, the RDSC works according to globally recognized standards and was accredited as a research data centre (RDC) by the German Data Forum(“Rat für Sozial- und Wirtschaftsdaten”).</p>
<h4 id="a-model-of-the-empirical-knowledge-generating-process">2.2 A model of the empirical knowledge generating process</h4>
<p>In this section, we first describe a model of the empirical knowledge generating process in a RDC and relate this model to the status quo. We then proceed by describing, how and more importantly where the outcome of the competition fits into this model. In a last step, we provide examples of how the outcome of the competition may help to improve the status quo. While we use the RDC research process as an example, we believe that the empirical knowledge generating process we present in this section is a fairly general model which is applicable to other institutional settings as well.</p>
<p>[Insert Figure 1]</p>
<p>Figure 1: Current information flows in empirical research</p>
<p>Currently, in the RDSC – after a successful approval process – access to microdata is given. In Figure 1, this data input to empirical research is labelled data services. Data services consist of the microdata itself and of information on data (metadata) from the data producing units in Bundesbank and the RDSC. With the help of these units, the RDSC documents, enriches and annotates data and facilitates access to microdata for research. The resulting product from data services thus goes beyond raw data. Refined, cleaned and linked data is provided, along with counsel if needed, as input to empirical research.</p>
<p>Researchers as users of micro data and data services (for example metadata), produce research outcomes. These outcomes – after data confidentiality clearance – sometimes take the form of publications. Publications contain knowledge accumulated by researchers about data usage over time (experience), e.g., knowledge about dataset particularities. Unstructured knowledge refers to information not written down, not shared outside of the current project, not machine-reusable or not understandable by the interest public. Examples of such knowledge acquired by researchers include:</p>
<ol type="1">
<li>How data is used (e.g. additional data cleaning, variable transformation, combining datasets, using additional information)</li>
<li>What purposes data is used for (e.g. topics, methodology, research area)</li>
<li>What kinds of analyses or techniques have been tried and are used ultimately</li>
<li>What information is most valuable to get the results.</li>
</ol>
<p>The challenge is to have a feedback loop from research and publications – as the users and outcome of microdata – back to data services to improve the data and/or the metadata (see Figure 2). At the moment, this feedback loop is not present in a systematic way. The aim of the competition was to identify appropriate procedures to close this gap, by transforming knowledge available in publications into generally re-usable knowledge to inform stakeholders (data producers, RDSC, decision makers at Bundesbank). The results of the competition will ultimately enable better data services which in turn will make research outcomes more efficient and better in general.</p>
<p>[Insert Figure 2]</p>
<p>Figure 2: Feedback generated knowledge back into research input</p>
<p>Important to note is that, while publications provide knowledge about research outcomes, results are presented to the interested public in a form optimized for human consumability as unstructured text. “User Specific Knowledge” in Figure 2 emphasizes knowledge that is findable and transformed for the different purposes. This requires unstructured text to be put into structured chunks of information for automatic classification and analyses. Only thereby can the information from the publication best be built upon. In order to get to such useful knowledge, we have to find (extract) and model (structure) information from research papers.</p>
<p>Summarizing, in this model the empirical knowledge generating process starts with data services. Research is conducted using such services. Researchers produce publications as the research outcome, from which information can be structured into analysable form. Data services can then be automatically and manually improved through feedback from the user-side. We arrive at the starting point (data services) but on a higher level. In turn better data services allow better research. By partly automatizing the feedback-loop between research, publications, knowledge, and data services, the knowledge generating circle can spin faster and be augmented more quickly. Thinking of the data-driven knowledge generating process, we consider a circle model that increases in levels.</p>
<h4 id="added-value-of-structured-user-specific-knowledge">2.3 Added value of structured user-specific knowledge</h4>
<p>Being able to access structured user-specific knowledge as described in the last section, enables improvin g data services by making discovery of data and related projects, people, and publications at Bundesbank more comprehensive and efficient. For example, knowledge harvested from publications may be used to enhance services provided by RDSC by allowing standard datasets to be tailored to the needs of researchers. Similarly, data producers benefit from feedback on their data, allowing them to improve data quality.</p>
<p>Structured user-specific knowledge produced during the competition may be used to inform the design of a dataset proposition system for researchers. By obtaining information on dataset usage in publications, data is for the first time available to construct indices on data set joint usability (and dataset maps to visualize such indices). Such an index connects datasets through actual use by researchers that combined data sets in the past. This enables recommendations, such as, <em>“Researchers, who used dataset A, also used dataset B”</em>.</p>
<p>Going further, the usability index can be expanded into a measure, how well new datasets fit each other. For this proposals are predicted based on dataset usage in the same field, using the same methods or by additional metadata similarity (hence without necessarily needing joint dataset usage in the past). This can be a valuable accelerator to effectively distribute new datasets in the research community. Both indices can be implemented using information from the competition only, however we propose extensions, based on other information such as current metadata, that may enhance value to users.</p>
<p>When thinking about user recommendations, the example is set by large online platforms. Online platforms can recommend from two dimensions of information (excluding interaction for simplicity). Data is available on a large number of observed purchases per customer, which enable statements like <em>“since you like products A and B, you might also like C”</em>. In the second dimension, data is present on large numbers of observed customers per product, which enables statements like “users like you also bought”.</p>
<p>In our setting with researchers and datasets, the universe of users/ researchers is decently large, but per user, we observe a limited amount of “dataset consumption”. Hence, we have a decent chance of recommending based on other users behaviour. However, we have only limited means of predicting a single users future needs based on his past direct “dataset shopping” behaviour.</p>
<p>As a hypothesis, we suspect a simpler underlying behavioural model of data shopping, because publishing with one dataset is not a casual purchase. It implies real commitment and also successful publishing, which relates to being content with the purchase (less cognitive dissonance). Thus, we would need less data points per person than online platforms, in order to make sensible recommendations. Also, in order to gain more of the rare information per user, we can fall back on dataset citations, i.e. “indirect data usage”, as outlined in Chapter 3.</p>
<p>The RDSC also has a responsibility towards its principals i.e. society as a public institution. Granting data access free of charge for researchers should be backed by empirical benefits of such data provision. Thereby societal investment in free data access can be justified. Societal benefits of data access can be measured in knowledge created by specific datasets. Knowing which datasets underlie new research findings (which datasets are used in which publication and which findings are enabled by the specific dataset), enables evaluating the added value of a dataset for Bundesbank or society.</p>
<p>Measuring societal benefits through data is not obvious at first glance. Judging the added value of dataset provision first requires identifying which empirical result from a publication can be attributed to which dataset. One can argue that added value of providing micro data is the marginal benefit compared to the second-best data, if comparable commercial databases exist. Also, one can argue that a dataset, which enables causal evidence, adds more value to societal knowledge, compared to previously available datasets, from which only correlations could be deduced if an important goal is to inform the policy debate. In its most extreme case, new datasets can allow entirely new research questions to be answered.</p>
<p>This section detailed two examples of value generated by the competition for the RDSC. First, systems can be optimized in a user-centric way, by obtaining refined products (e.g.  improved researcher recommendations and data documentation). Second, with dataset usage data, the case for societal investment in free data access can be empirically fortified. Positive externalities (research as a public good) would currently suggest a less then societally optimal provision of processes and institutions that generate research data and offer related services. In the next section, we proceed by describing what we learned in the competition on the road to data-driven systems.</p>
<h3 id="lessons-learned-from-competition">3. Lessons learned from competition</h3>
<h4 id="dataset-mentions">3.1 Dataset mentions</h4>
<p>This section presents lessons we learned throughout the duration of the competition. We organise this section around the three sets of information that where the main focus of the competition: datasets mentions, research fields, and (statistical) methods used. We begin by describing our a priori expectation of what a dataset is. We did not delve into definitions of a dataset but rather considered it sufficiently defined for our purposes (as empirical social scientists and for the competition).</p>
<p>Since our approach depends on getting to know the user-perspective, we thought it plausible to let usage in empirical papers define a dataset for the purpose of the competition. Having a background in working at a large provider of financial data, we had a vague idea that all datasets would look like those the RDSC provides access to, which consist mostly of collections of structured data in matrix or database form. These datasets typically are defined by a name and with a well-defined scope, thus allowing clear citation, probably including a unique dataset identifier (such as a Digital Object Identifier, DOI).</p>
<h5 id="lesson-1-datasets-fall-into-two-broad-categories">Lesson #1: datasets fall into two broad categories</h5>
<p>Since the corpus of publication used for the competition spanned different domains (like healthcare, education, and others), we quickly realized that our dataset image had an econocentric bias. In social science, we learned, datasets can be categorized into two broad categories for the purposes of extraction. First, there are named datasets, i.e. well defined, usually large-scale and publicized datasets (e.g. Compustat).</p>
<p>Generally, named dataset mentions are short strings in the publications, have commonly used abbreviations (e.g. MMSR), and often containing institution name or name of commercial data vendor. Sometimes (rarely, but increasingly) these datasets can be identified by a unique digital object identifier (DOI). These datasets are usually well-defined in scope and time, with formal documentation available. While data is usually collected with a specific purpose in mind, such datasets are be used across multiple papers and research domains.</p>
<p>The second dataset category is what we call created datasets. By created dataset we understand datasets usually collected or built by authors of a publication for the purpose of analysing one specific research question. Often, created data comes in the form of small-scale surveys, (structured) interviews, or randomized controlled trials, RCTs. Such data normally does not have a trademark name, but instead one or multiple paragraph descriptions in the publication. Dataset information is blended together with information on data collection and sampling methods. Data reference at its most condensed form then comes in a structure like <em>“we interview a given number of participants in a given region suffering from a given disease and code responses in the following way”</em>.</p>
<p>In contrast to named datasets, created datasets usually are not referred to by a specific string or commonly used abbreviation. Data collection is usually paper specific, and the universe of existing datasets are not easily searchable. This makes it hard for text mining algorithms to correctly extract strings referring to dataset entities. Specific created datasets are harder to use for follow-up research, and reproducibility is given only if publishers provide data together with the paper. Therefore, the lack of unique identification and search terms renders data collection potentially redundant and dataset spread not optimal.</p>
<h5 id="lesson-2-fractions-of-dataset-category-are-domain-specific">Lesson #2: Fractions of dataset category are domain specific</h5>
<p>Throughout the competition duration it became clear that the fraction of named and created datasets varies across social science domains. Since different fields of social sciences rely on different identification techniques and differing potentials for conducting RCTs, the predominantly used data sources naturally vary. This has important repercussions for designing a competition, since algorithm performance and later recommendation system performance varies with the input corpus and the application field.</p>
<p>The number of datasets used per empirical paper (linked data) also varies across research areas. This number is also dependent on named vs. created datasets. In fields with widespread use of multiple datasets at once, the added value of recommending additional useful data might be expected to be higher than in fields that create study-specific data every time. Conversely, one could argue that the marginal utility of adding additional datasets is decreasing.</p>
<p>The optimal way forward is to start a data recommendation system for research field with higher expected marginal utility from additional datasets. In our view, these are research areas with widespread usage of named datasets. Named datasets are constructed without the concrete research question in mind. That is why information to answer a particular research question often has to be obtained from more than one data source and is particularly true in empirical economic and finance research.</p>
<h5 id="lesson-3-unique-identification-of-datasets-remains-an-issue">Lesson #3: Unique identification of datasets remains an issue</h5>
<p>From the distinction above, one could make the argument that named datasets are easier to identify than created datasets. However, this is not the case, because the same dataset name can refer to multiple subsamples or waves of same datasets, and it is unclear where to make distinctions between dataset entities. This makes it difficult to identify the mentions referring to the same data points. Issues are, just to name a few, different time periods or subsamples, different states of data and states of knowledge, computational data pre-processing or enrichment steps. These identification issues render the current task of entity resolution of extracted dataset mentions complicated.</p>
<p>Unique dataset identification carries significant repercussions for reproducibility purposes, where identifying the exact data used for a study is paramount. For reproducibility purposes, the current solution to this dataset identification problem is the direct data upload to the publisher together with the publication. This is neither storage-efficient for large datasets nor feasible in the case of confidential microdata. A more flexible way to solve this issue is to assign unique identifiers (DOIs) to the datasets.</p>
<p>With a DOI (identifying the exact time frame, sampling universe, data version, wave, aggregations, state of knowledge, etc.), datasets are identified and quantitative research using confidential microdata is reproducible. To make lives easier, DOIs also drastically facilitate the automatized extraction of well-defined datasets from publications (comparable to largely standardized citations of other publications, allowing easy retrieval of publication networks, etc.).</p>
<p>Summarizing, if we successfully identify datasets and solve the issue of entity resolution, we can link and propose created datasets and thereby enable further research with such data, which takes up a notable fraction of publications in certain fields. While this task is harder than for named datasets, the potential for improvement remains larger as of today. For created datasets, too, DOI usage would be desirable; however encouragement or enforcement to use DOIs is harder in this case, because of a larger target group – authors instead of a limited number of data stewards. Even in case of widespread DOI usage for named datasets, the competition algorithms yield valuable results through the created datasets extraction in order to allow referencing and making available datasets used in the past for further analysis.</p>
<h5 id="lesson-4-datasets-mentions-could-indicate-used-for-analysis-vs.-cited">Lesson #4: Datasets mentions could indicate used for analysis vs. cited</h5>
<p>After a discussion about dataset types and usage in fields, the last lesson that we learned about datasets concerns the mention of datasets in publications. These mentions come in two types. First, datasets used for empirical analysis and second, cited datasets in the literature review or references. Dataset citations (without empirical usage) can generally occur in the literature review section, even in theoretical, methodological papers, e.g. a given paper might report summary statistics based on datasets (<em>“Author Y uses Compustat to…”</em>). Sometimes differences between cited and used datasets are only semantic in nature. In well-written papers, the difference is usually fairly easy to distinguish for humans, but less clear for algorithms.</p>
<p>A key lesson we learned, is to think ahead of time, what the informational need is for the use-case at hand, used or cited datasets. Note that in an optimal setting, if information were available on the universe of datasets used for analysis in papers and on all publication citations, dataset citations would be redundant. This comes from the fact that a dataset citation in one publication is based on a dataset used for analysis in another publication and can be linked via available literature citations.</p>
<p>While literature citations are mostly standardized within research domains and are relatively straightforward to extract (hence publication networks / publications maps exist), information on used datasets in papers remains incomplete (even after the competition). Because of this, for the competition, we asked for used and cited datasets. It is important to note, that extracted dataset citations are always incomplete, since some authors report aggregate statistics from a different paper, but not the data behind (<em>“Smith et al show…”</em>).</p>
<p>If well separated, through extracted dataset citations, one obtains a “dataset map”, thus the “closeness of datasets”, and network measures such as centrality distinguishing important datasets (“nodes”). Through extracted empirical dataset usage on the other hand, one obtains relevant information for our purposes, namely information relating to dataset similarity and joint usage possibilities from the user perspective. However, for our envisioned recommendation system, usage of cited data (“indirect” data usage) is a valuable feature, since it yields more limited data on dataset “purchases” of a user.</p>
<p>As training data for the algorithms it is important to include theoretical literature, essays, etc. in the corpus of publications. Obviously, this is helpful for algorithms to correctly identify true negatives, i.e. correctly identifying theoretical papers. For this task, distinguishing between cited and used datasets becomes relevant once again, because clearly separating theoretical papers that merely cite data from empirical papers depend on such a distinction.</p>
<h4 id="fields-and-methods">3.2 Fields and Methods</h4>
<p>The competition also asked participants to extract information about research fields and methods used in the publication. We want to gather this information from the user side, because data producers and annotators do not necessarily foresee all usage potential for their data and the point of our envisioned system is to increase user value. One such idea is to construct dataset similarity indices from the usage side, information is relevant not only on existing joint usage by others (<em>“people like you often used dataset Y, too”</em> – hence dataset extraction), but also on new dataset or linkage potentials (<em>“this might also interest you based on your preferences”</em>). For this, information is necessary on the context, how datasets are used.</p>
<h5 id="lesson-5-think-before-you-act-define-fields-and-methods">Lesson #5: Think before you act: define fields and methods</h5>
<p>To obtain the most relevant categories of research fields, we did not provide any thesauri to the competition, on purpose. The rationale behind this was to see the unhindered creativity of teams, which available information sources they would use or not use (e.g. reference datasets, Wikipedia, archive.org, other repositories, thesauri, statistical clustering techniques, etc.). On the other hand, thesauri limit the catalogue of potentially identifiable fields and methods, thus prohibiting new methods and fields to be identified in fast-changing modern research areas. Also thesauri might disturb algorithm performance, since algorithm might be forced to categorize topics and fields to older or less exact categories than necessary.</p>
<p>However, using thesauri does have well-known advantages, as any librarian will confirm. These advantages include easy clustering of similar fields and methods and a manageable category set of predictions. For field predictions, we generally face a fine line between too broad predictions (safe, but uninformative) and too narrow predictions (narrow, but potentially wrong). A potential way out is backward induction here – we can present differently aggregated predictions for fields to users and get feedback from them (let users rank usability – <em>“Was this helpful to you?”</em>).</p>
<p>Concerning our definition of methods for the purpose of the competition, two questions arise. The first is the definition of statistical methods (i.e. inclusion of sampling methods, qualitative methods, etc.). Secondly, there are multiple statistical methods in a publication (besides the main causal analysis, there can be methods reported for data preparation, sampling, baseline results, robustness checks, descriptive statistics, etc.) and issues of potential weighting of importance of these.</p>
<p>For useful new recommendations to be provided to researchers, we decide to include in statistical methods all methods that describe potential for a merge of datasets / joint usability, hence to include all the above listed. We consider a broad definition of methods, not only including high-level statistical methods, such as ordinary least squares, but also including the observed unit, time period or even regression equations. If two papers then use different datasets in the same field using the same methods, there is a relatively high likelihood that those datasets can be linked or used together to create new insights.</p>
<h4 id="next-steps">3.3 Next steps</h4>
<p>Several decades ago, publication citation networks were constructed and – to our knowledge - no such undertaking has yet been done for datasets. This comes from the fact that no curated training data corpus is readily available in decent quality. Since no such data is available, we manually annotate papers for the competition and now propose to go forward with this in a larger scale.</p>
<p>We would have no need for this competition in a world with universal dataset identifier usage (such as DOIs). In such a scenario unique identification and standardized citations of datasets would be readily available. Since DOIs only now and slowly gain widespread application for datasets in social science, our task is a 1:n mapping of publications to datasets without unique identifiers. For scientific papers many journals already provide DOIs for papers.</p>
<p>There are ongoing efforts by journals to have all used data published for reproducibility reasons. Incentivizing researchers to provide unique identification of datasets used in papers is a logical next step. This will ensure reproducibility for confidential microdata and facilitate our use-cases. In the meantime, we show a way forward to learn from the current state of information and analytically use presently available information.</p>
<p>The competition highlights that datasets can be categorized in different dimensions for the purposes of extracting dataset mentions from publications. We propose a binary distinction of datasets into named as opposed to created datasets. As named datasets, we consider formal, large datasets by commercial or official institutions, often referenced in relatively standardized forms as commonly used abbreviations. Created datasets are those created for the specific purpose of one research question in mind. They are generally described in less standardized paragraphs. Usage of named versus created datasets varies across research areas.</p>
<p>Also varying across research areas is the number of datasets used per empirical paper. This number also depends on the spread of formal, named datasets as opposed to created datasets for single studies. In fields with widespread use of multiple datasets at once (linked data), the added value of recommending additional useful data might be expected to be higher than in fields that create study-specific data every time. Conversely, one could argue that the marginal utility of adding additional datasets is decreasing. The optimal way forward is to start a data recommendation system for research field with higher expected marginal utility from additional datasets.</p>
<h3 id="conclusions">4. Conclusions</h3>
<p>In this competition, we asked teams to extract datasets, fields and methods from a corpus of hand-annotated research publications. The value of the extracted information lies in informing a user-centric dataset recommendation system and thereby enabling optimal and timely spread of available datasets throughout the research community. Furthermore, such information allows us to compute dataset impact factors by obtaining data-driven information on which datasets underlie high-quality research outputs. This in turn is a proxy for societal benefits of data provision by research data centres, thus motivating investment in data access infrastructure.</p>
<p>We introduce a circular model of the knowledge generating process, which increases in levels. From data services, research is conducted, publications are published and user-specific knowledge is generated. Having such knowledge on dataset usage, data services in turn can be improved. Thereby the circle repeats on a higher level. The current competition works on strengthening the knowledge pillar as well as the transmission mechanisms from publications to knowledge to improved data services.</p>
<p>Automatic processing of generated knowledge in publications becomes increasingly available with modern text analysis tools. Extracting such information is important, because timely and optimal usage of gained results increases the speed, by which findings can be incorporated into data services and thereby next-level research is enabled in turn. To further improve automatic processing, minimum standards for dataset taxonomy are needed. Harmonized metadata schemas for data sets – like the INEXDA metadata schema (Bender et al. 2018) for central banks and statistical offices (compliant with and building upon DataCite) – offer such an approach.</p>
<p>The competition showcased that information extraction of the necessary information for such systems is possible. The delivered prototype algorithms prove this claim. With the proof of concept, there is a more substantiated case for investing in a larger hand-curated training corpus of annotated research papers. On the road towards a user-centric dataset recommendation and metadata system, the competition forced us to clarify organizational needs and methodological aspects.</p>
<p>For the way forward, it is important to note the importance of the research area on the strategic path towards a unified user-centric microdata recommendation system. The choice of the research domain will greatly influence algorithm performance. Since human effort in creating training data is expensive, one should deliberately pick research domains to start with. This arises because text extraction algorithms (and humans) struggle with informally described created datasets. The low-hanging fruits of prototyping dataset recommendation systems, usability indices etc. are easier to implement for research areas with a largely formalized dataset citation culture (however ultimately potential for benefits may well be larger in other research areas).</p>
<h3 id="references">5. References</h3>
<ul>
<li>Ball, A., and M. Duke (2011): How to cite datasets and link to publications. Digital Curation Centre.</li>
<li>Bender, S., Hausstein, B., &amp; C. Hirsch (2018): An Introduction to INEXDA’s Metadata Schema. Technical Report 2018-02, Deutsche Bundesbank, Research Data and Service Centre.</li>
<li>Boland, K., Ritze D., Eckert, K., &amp; B. Mathiak (2012): Identifying references to datasets in publications. Theory and Practice of Digital Libraries, pp. 150-161. Springer Berlin Heidelberg, http://doi.org/10.1007/978-3-642-33290-6_17.</li>
<li>Ghavimi, B., Mayr, P., Vahdati, S., &amp; C. Lange (2016): Identifying and improving dataset references in social sciences full texts. arXiv preprint arXiv:1603.01774.</li>
<li>Helbig K., Hausstein B., Koch U., Meichsner J., &amp; A. Kempf (2014): da|ra Metadata Schema. Gesis Technical Reports 2014/17, DOI:10.4232/10.mdsdoc.3.1.</li>
<li>Koesten, L., Mayr, P., Groth, P., Simperl, E., &amp; M. de Rijke (2019): Report on the DATA: SEARCH’18 workshop-Searching Data on the Web. ACM SIGIR Forum (Vol. 52, No. 1, pp. 117-124). ACM.</li>
<li>Boland, K. &amp; B. Mathiak (2015). Challenges in Matching Dataset Citation Strings to Datasets in Social Science. D-Lib Magazine 21, 1/2.</li>
<li>McMurry, J. A., Juty, N., Blomberg, N., Burdett, T., Conlin, T., Conte, N., &amp; A. Gonzalez-Beltran, A. (2017): Identifiers for the 21st century: How to design, provision, and reuse persistent identifiers to maximize utility and impact of life science data. PLoS biology, 15(6), e2001414.</li>
<li>Mooney, H, &amp; M. P. Newton (2012): The anatomy of a data citation: Discovery, reuse, and credit. eP1035-eP1035.</li>
<li>Vilhuber, L. &amp; C. Lagoze (2017): Making Confidential Data Part of Reproducible Research, Chance.</li>
<li>Zhang, Q., Cheng, Q., Huang, Y., &amp; W. Lu (2016): A bootstrapping-based method to automatically identify data-usage statements in publications. Journal of Data and Information Science, 1(1), 69-85.</li>
</ul>
<p>Placeholder for Dimensions use case chapter. Placeholder for Bob Allen chapter on broader information context. Placeholder for the competion design chapter.</p>
<h1 id="introduction-1">Introduction</h1>
<p>The Allen Institute for Artificial Intelligence (AI2) is a non-profit research institute founded by Paul G. Allen with the goal of advancing artificial intelligence research for the common good. One of the major undertakings at AI2 is to develop an equitable, unbiased software platform (Semantic Scholar)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> for finding relevant information in the scientific literature. Semantic Scholar extracts meaningful structures in a paper (e.g., images, entities, relationships) and links them to other artifacts when possible (e.g., knowledge bases, GitHub repositories), hence our interest in the rich context competition (RCC). In particular, we participated in the RCC in order to explore methods for extracting and linking datasets used in papers. At the time of this writing, Semantic Scholar comprehensively covers the computer science and biomedical literature, but we plan to expand our coverage in 2019 to other scientific areas, including social sciences.</p>
<p>In the following sections, we describe our approach to the three tasks of the RCC competition: extracting datasets used in publications (Section <a href="#sec:datasets" data-reference-type="ref" data-reference="sec:datasets">2</a>), research area prediction (Section <a href="#sec:areas" data-reference-type="ref" data-reference="sec:areas">3</a>) and research method extraction (Section <a href="#sec:methods" data-reference-type="ref" data-reference="sec:methods">4</a>).</p>
<h1 id="sec:datasets">Dataset Extraction and Linking</h1>
<p>This task focuses on identifying datasets used in a scientific paper. Datasets which are merely mentioned but not used in the research paper are not of interest. This task has two sub-tasks:</p>
<ol type="1">
<li><p>Citation prediction: extraction and linking to a provided knowledge base of <em>known datasets</em>, and</p></li>
<li><p>Mention prediction: extraction of both <em>unknown and unknown</em> dataset mentions.</p></li>
</ol>
<h4 id="provided-data.">Provided Data.</h4>
<p>The provided knowledge base of known datasets includes approximately 10K datasets used in social science research. Many of the datasets in the knowledge base are specific years or sections of larger surveys, e.g.,</p>
<ul>
<li><p>Monitoring the Future: A Continuing Study of the Lifestyles and Values of Youth, 1980</p></li>
<li><p>Monitoring the Future: A Continuing Study of the Lifestyles and Values of Youth, 1983</p></li>
<li><p>Monitoring the Future: A Continuing Study of American Youth (12th-Grade Survey), 1996</p></li>
</ul>
<p>The high textual similarity between different datasets in the knowledge base makes the linking task more challenging.</p>
<p>In the first phase of the RCC, organizers provided participants with 5K papers, partially annotated with dataset usage to serve as training data. For each paper, the full text, metadata, and PDF file were provided. For each annotation, the paper ID and the corresponding dataset ID in the knowledge base were provided. For most annotations, the textual mentions in the paper were also provided, but the position of the mention in the paper text was not specified. This means that for a mention that appears multiple times in a paper, it is ambiguous which of these mentions is actually the reference to the dataset that has been labeled. In order to actually label the text in the paper, we need to search the paper for the annotated mention, and if the mention appears multiple times, it is not clear which of these are valid examples of dataset usage in a paper. Approximately 10% of the datasets in the knowledge base were linked one or more times in the provided corpus of 5K papers.</p>
<figure>
<img src="datasets.png" alt="image" style="width:13cm" /><figcaption>image</figcaption>
</figure>
<p>We provide a high-level overview of our approach in Figure <a href="#fig:datasets" data-reference-type="ref" data-reference="fig:datasets">[fig:datasets]</a>. First, we use a named entity recognition (NER) model to predict dataset mentions. For each mention, we generate a list of candidate datasets from the knowledge base. We also developed a rule based extraction system which searches for dataset mentions seen in the training set, adding the corresponding dataset IDs in the training set annotations as candidates. We then use a binary classifier to predict which of these candidates is a correct dataset extraction.</p>
<p>Next, we describe each of the sub-components in more detail.</p>
<h4 id="mention-and-candidate-generation.">Mention and Candidate Generation.</h4>
<p>We first constructed a set of rule based candidate citations by exact string matching mentions and dataset names from the provided knowledge base. We found this to have high recall on the provided development fold and our own development fold that we created. However, after our test submission, it became clear that there were many datasets in the actual test set that did not have mentions in the provided knowledge base.</p>
<p>To address this limitation, we developed an NER model to predict additional dataset mentions. For NER, we use a bi-LSTM model with a CRF decoding layer, similar to <span class="citation" data-cites="Peters2018DEEPCW">[@Peters2018DEEPCW]</span>, and implemented using the AllenNLP framework.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> In order to train the NER model, we automatically generate mention labels by string matching mentions in the provided annotations against the full text of a paper. This results in noisy labeled data, because it was not possible to find all correct mentions this way (e.g., some dataset mentions were not annotated), and the same string can appear multiple times in the paper, while only some are correct examples of dataset usage.</p>
<p>We limit the percentage of negative examples (i.e., sentences with no mentions) used in training to 50%, and use 40 words as the maximum sentence length. We use 50-dimensional Glove word embeddings <span class="citation" data-cites="Pennington2014GloveGV">[@Pennington2014GloveGV]</span>, 16-dimensional character embeddings with 64 CNN filters of sizes (2, 3, 4). The CNN character encoder outputs 128-dimensional vectors. We optimize model parameters using ADAM <span class="citation" data-cites="Kingma2014AdamAM">[@Kingma2014AdamAM]</span> with a learning rate of 0.001.</p>
<p>In order to generate linking candidates for the NER mentions, we score each dataset based on TF-IDF weighted token overlap between the mention text and the dataset title. For a given mention, many dataset titles can have a non-zero overlap score, so we take the top 30 scoring candidates for each mention as the linking candidates for that mention.</p>
<h4 id="candidate-linking.">Candidate Linking.</h4>
<p>The linking model takes as input a dataset mention, its context, and one of the candidate datasets in the knowledge base, and outputs a binary label. We use a gradient boosted trees classifier using the XGBoost implementation.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> We use the following features: prior probability of entity, prior probability of entity given mention, prior probability of mention given entity, whether a year appears in the mention context and in the dataset title, mention length, mention sentence length, whether the mention is an acronym, estimated section title of the mention, overlap between mention context and dataset keywords provided in the knowledge base, and the TF-IDF weighted token overlap. We note that it is possible to predict zero, one or multiple dataset IDs for the same mention.</p>
<h4 id="results.">Results.</h4>
<p>First, we report the results of our NER model in Table <a href="#tab:ner_results" data-reference-type="ref" data-reference="tab:ner_results">[tab:ner_results]</a>. Since it is easy for the model to memorize the dataset mentions seen at training time, we created disjoint train, development, and test sets based on the paper–dataset annotations provided for the competition. In particular, we sort datasets by the number of papers they appear in, then process one dataset at a time. For each dataset, we choose one of the train, development or test splits at random and add the dataset to either the train, development or test sets, along with all papers which mention that dataset. When there is a conflict, (e.g., a paper <span class="math inline"><strong>p</strong></span> has already been added to the train split when processing an earlier dataset <span class="math inline"><strong>d</strong><sub><strong>1</strong></sub></span>, but it is also associated with a later dataset <span class="math inline"><strong>d</strong><sub><strong>2</strong></sub></span>), the later dataset <span class="math inline"><strong>d</strong><sub><strong>2</strong></sub></span> along with all papers associated with it are added to the same split as <span class="math inline"><strong>d</strong><sub><strong>1</strong></sub></span>. For any further conflicts, we prefer to put papers in the development split over the train split, and the test split over the development split.</p>
<p>We also experimented with adding ELMo embeddings <span class="citation" data-cites="Peters2018DEEPCW">[@Peters2018DEEPCW]</span>, but it significantly slowed down training and decoding which would have disqualified our submission due to the runtime requirements of the competition. As a result, we decided not to include ELMo embeddings in our final model.</p>
<table>
<caption>NER precision, recall and F1 performance (%) on the development and test sets.</caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">prec.</th>
<th style="text-align: center;">recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dev set</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr class="even">
<td>test set</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">45.8</td>
</tr>
</tbody>
</table>
<p><span id="tab:ner_results" label="tab:ner_results">[tab:ner_results]</span></p>
<table>
<caption>End-to-end precision, recall, and F1 performance (%) for dataset prediction on the phase 1 and phase 2 holdout sets. Note that the phase 1 holdout results are for citation prediction, while the phase 2 holdout results are for mention prediction.</caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">prec.</th>
<th style="text-align: center;">recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>phase 1 holdout</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">25.3</td>
</tr>
<tr class="even">
<td>phase 2 holdout</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">25.5</td>
</tr>
</tbody>
</table>
<p><span id="tab:test_results" label="tab:test_results">[tab:test_results]</span></p>
<p>We report the end-to-end performance of our approach (on the development set provided by the organizers in the first phase) in Table <a href="#tab:e2e_results" data-reference-type="ref" data-reference="tab:e2e_results">[tab:e2e_results]</a>. This is the performance after using the linking classifier to predict which candidate mention, dataset pairs are correct extractions. We note that the development set provided in phase 1 ended up having significantly more overlap with the training data than the actual test set did. As a result, the numbers reported in Table <a href="#tab:e2e_results" data-reference-type="ref" data-reference="tab:e2e_results">[tab:e2e_results]</a> are not indicative of test set performance. End to end performance from our phase 2 submission can be seen in Table <a href="#tab:test_results" data-reference-type="ref" data-reference="tab:test_results">[tab:test_results]</a>. This performance is reflective of our focus on the linking component of this task. Aside from the competition development set, we also used a random portion of the training set as an additional development set. The initial model only uses a dataset frequency feature, which gives a baseline performance of 38.4 F1. Adding p(d <span class="math inline">∣</span> m) and p(m <span class="math inline">∣</span> d), which are the probability of entity given mention and probability of mention given entity improves the performance (<span class="math inline"><em>Δ</em> = 2.3</span> F1). Year matching helps disambiguate between different datasets in the same series, which was found to be a major source of errors in earlier models (<span class="math inline"><em>Δ</em> = 2.8</span> F1). Aggregating mentions for a given dataset, adding mention and sentence length features, adding an is acronym feature, and further hyper-parameter tuning improve the results (<span class="math inline"><em>Δ</em> = 12.5</span> F1). Adding examples in the development set while training the model results in further improvements (<span class="math inline"><em>Δ</em> = 2.8</span> F1). Finally, adding the NER-based mentions significantly improves recall at the cost of lower precision, with a positive net effect on F1 score (<span class="math inline"><em>Δ</em> = 0.7</span> F1).</p>
<table>
<caption>End-to-end precision, recall and F1 performance (%) for citation prediction on the development set provided in phase 1 of the competition.</caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">prec.</th>
<th style="text-align: center;">recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>baseline</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">38.4</td>
</tr>
<tr class="even">
<td>+ p(d <span class="math inline">∣</span> m), p(m <span class="math inline">∣</span> d)</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">40.7</td>
</tr>
<tr class="odd">
<td>+ year matching</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">43.5</td>
</tr>
<tr class="even">
<td>+ aggregated mentions, tuning</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td>and other features</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr class="even">
<td>+ dev set examples</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr class="odd">
<td>+ NER mentions</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">59.0</td>
</tr>
</tbody>
</table>
<p><span id="tab:e2e_results" label="tab:e2e_results">[tab:e2e_results]</span></p>
<h1 id="sec:areas">Research Area Prediction</h1>
<h4 id="data.">Data.</h4>
<p>The second task of the competition is to predict research areas of a paper. The task does not specify the set of research areas of interest, nor is training data provided for the task. After manual inspection of a subset of the papers in the provided test set, the SAGE taxonomy of research, and the Microsoft Academic Graph (MAG) <span class="citation" data-cites="Shen2018AWS">[@Shen2018AWS]</span>, we decided to use a subset of the fields of study in MAG as labels. In particular, we included all fields related to social science or papers from the provided training corpus. However, since the abstract and full text of papers are not provided in MAG, we only use the paper titles for training our model. The training data we ended up with included approximately 75K paper titles along with their fields of study as specified in two levels of the MAG hierarchy. We held out about 10% of the titles for development data. The coarse level (L0) has 7 fields while the more granular one (L1) has 32. Fields associated with less than 100 papers were excluded.</p>
<h4 id="methods.">Methods.</h4>
<p>For each level, we trained a bi-directional LSTM which reads the paper title and predicts one of the fields in this level. We additionally incorporate ELMo embeddings <span class="citation" data-cites="Peters2018DEEPCW">[@Peters2018DEEPCW]</span> to improve performance. In the final submission, we always predict the most likely field from the L0 classifier, and only report the most likely field from the L1 classifier if it exceeds a certain threshold. It takes approximately 1.5 and 3.5 hours for the L0 and L1 classifiers to converge, respectively.</p>
<h4 id="results.-1">Results.</h4>
<p>To select a model, we performed a 100 trial random search across model hyper-parameters, evaluated on a held out development set of papers from the Microsoft Academic Graph. Our final model contained 512 hidden dimensions, 2 layers and 0.5 dropout prior to classification. The top performing classifier achieved 84.4% accuracy on our development set on L0 fields, and 65.2% accuracy on our development set on L1 fields.</p>
<h1 id="sec:methods">Research Method Extraction</h1>
<h4 id="data.-1">Data.</h4>
<p>The third task in the competition is to extract the scientific methods used in the research paper. Since no training data was provided, we started by inspecting a subset of the provided papers to get a better understanding of what kind of methods are used in social science and how they are referred to within papers.</p>
<h4 id="methods.-1">Methods.</h4>
<p>Based on the inspection, we designed regular expressions which capture common contextual patterns as well as the list of provided SAGE methods. In order to score candidates, we used a background corpus to estimate the salience of candidate methods in a paper. Two additional strategies were attempted but proved unsuccessful: a weakly-supervised model for named entity recognition, and using open information extraction (openIE) to further generalize the list of candidate methods.</p>
<h4 id="results.-2">Results.</h4>
<p>We evaluated performance by manually evaluating the output of our extractor for a subset of 50 papers from the provided test set to compute precision. Since evaluating recall requires a careful annotation, we resorted to using yield as an alternative metric. Our final submission for method extraction has a 95% precision and yield of 1.5 methods per paper on the manually inspected subset of papers.</p>
<h1 id="conclusion">Conclusion</h1>
<p>This report summarizes the AI2 submission at the RCC competition. We identify dataset mentions by combining the predictions of an NER model and a rule-based system, use TF-IDF to identify candidates for a given mention, and use a gradient boosted trees classifier to predict a binary label for each candidate mention, dataset pair. To identify research fields of a paper, we train two multi-class classifiers, one for each of the top two levels in the MAG hierarchy for fields of study. Finally, we use a rule based system utilizing a dictionary and common patterns, followed by a scoring function which takes into account the prominence of a candidate in foreground and background corpora.</p>
<h1 id="future-work">Future Work</h1>
<p>We now provide some possible directions of improvement for each component of our submission. For dataset extraction, the most promising avenue of improvement is to improve the NER model, and the most promising avenue to improve the NER model is to collect less noisy data. We effectively have distantly supervised training data for the NER model, and the first thing to try would be directly annotating papers with dataset mentions to provide a clearer signal for the NER model. For research area prediction, it would help to include signals beyond just the paper title for predicting the field of study. The difficulty here is finding labeled training data that includes richer signals like abstract text and paper keywords. For method prediction, further exploration of using open information extraction is a potential avenue of future research. Additionally, it would be helpful to clarify what exactly is meant by a method, as it is currently unclear what a successful method extraction looks like.</p>
<h1 id="acknowledgments" class="unnumbered">Acknowledgments</h1>
<p>We would like to thank the competition organizers for their tireless efforts in preparing the data, answering all our questions, doing the evaluations, and providing feedback. We also would like to thank Zhihong (Siri) Shen for helping us use the MAG data.</p>
<h1 id="introduction-2">Introduction</h1>
<p>Datasets are very valuable resources when doing research. However, it is not easy to find who else worked with that data, on what topics and with what results. Therefore, there is a waste of time and resources looking for related works using a specific dataset and what is more, some useful publications are underused. This work is the first step to tackle this problem. We proposed a model to retrieve dataset mentions, research fields and research methods from scientific publications. Our main approach to solve them is reading comprehension, named entity recognition, and TF-IDF similarity.</p>
<h1 id="related-works">Related works</h1>
<p><strong>Datasets retrieval</strong> Although <em>Information Retrieval</em> is a well established research field, only few attempts have focused on the task of dataset extraction form publications. <span class="citation" data-cites="ghavimi2016identifying">[@ghavimi2016identifying]</span> tried it using heuristics and dictionaries but their heuristics have some problems. Firstly, they give too much weight to acronyms. For example, <em>NYPD, (New York Police Department)</em> is detected as a dataset name. Furthermore, they also give an too much weight to the publication year of the datasets because they assumed dataset names are usually followed by the publication year but that may only work on Social Sciences publications. For example, in the Computer Science datasets do not appear followed by the publication year so this heuristic cannot detect all kind of dataset mentions.</p>
<h1 id="models">Models</h1>
<p>In this section, we will explain about the models we use for datasets retrieval, research fields retrieval, and research methods retrieval.</p>
<h2 id="datasets-retrieval">Datasets Retrieval</h2>
<p>Our approach to solve the dataset retrieval task is reading comprehension (RC) with query generation and entity typing. An RC model is applied to the given publications with our own query generation module. Then, the result from the RC model is filtered with an entity typing module. Figure <a href="#fig:%20docqaarch" data-reference-type="ref" data-reference="fig: docqaarch ">[fig: docqaarch ]</a> shows our overall approach for dataset retrieval. In following subsections RC model, query generation, and entity typing are explained in detail.</p>
<figure>
<img src="%7Bdocqaarch3%7D" alt="image" style="width:16cm" /><figcaption>image</figcaption>
</figure>
<h3 id="document-qa">Document QA</h3>
<p>Reading comprehension models are neural networks that find answers for given queries according to a text. Answers must appear explicitly in the text. Since the dataset retrieval task is about finding explicit dataset mentions from publications, RC models are suitable for this task.</p>
<p>The RC model used in this work is Document QA <span class="citation" data-cites="clark2017simple">[@clark2017simple]</span>. It uses Bi-GRU, bi-attention, and self-attention mechanism. In addition, Document QA performs a paragraph selection that pre-filters and selects the <em>k</em> most relevant paragraphs through TF-IDF similarity between the query and paragraphs. We observed that datasets are usually mentioned together in some specific paragraphs of the publications. Therefore, this model is appropriate for this task thanks to its paragraph selection stage.</p>
<h3 id="sec:questiongen">Query generation module</h3>
<p>In order to apply an RC model (such as Document QA) to the dataset retrieval task, queries that are suitable for finding the datasets are required. However, defining a general query for retrieving datasets is difficult, since the dataset mentions appear in various form like surveys, datasets, or studies. Therefore, we devised a query generation module with some important query terms to generate multiple specific queries instead of one general query.</p>
<p>To generate important query terms, we used a query generation model that creates queries given answers proposed by <span class="citation" data-cites="yuan2017machine">[@yuan2017machine]</span>. Thanks to this model, we could obtain a list of queries to retrieve datasets from the training set. After that, we extracted query terms that are frequent in the list of queries and at the same time are not frequent in non-dataset-mention sentences. Because of this, these query terms have discrimination power for retrieving dataset mentions since 1) queries are generated to extract mentions and 2) the query terms do not appear in the sentences without dataset mentions.</p>
<p>This list of query terms is used to generate a general query concatenating query terms. This query is used for the paragraph selection stage of Document QA, as shown in figure 1. After this stage, the query generation module generates queries for each paragraph by string matching, in order to create specific queries for each paragraph.</p>
<h3 id="ultra-fine-entity-typing">Ultra-Fine Entity Typing</h3>
<p>Ultra-Fine Entity Typing <span class="citation" data-cites="Choi:2018:ACL">[@Choi:2018:ACL]</span> can predict a set of free-from phrases like <em>criminal</em> or <em>skyscraper</em> given a sentence with an entity mention. For example, in the sentence: <em>Bob robbed John and he was arrested shortly afterwards</em>, Bob is of type <em>criminal</em>. In our task, candidate answers proposed by Document QA and their context are input to Ultra-Fine Entity Typing. Although this system can predict 10k different entity types in which <em>dataset</em> is included, after a few experiments we observed that most of the dataset names are recognized as some specific entity types such as <em>organization</em> and <em>agency</em>. Since these entity types are consistent, we decided that this could be a feature for our candidate answer classifier.</p>
<h3 id="candidate-answer-classifier">Candidate Answer Classifier</h3>
<p>Using the score given by RC model for each candidate answer and the entity types given by Ultra-Fine Entity Typing for each candidate answer, a neural network classifier that filters the candidate answers of Document QA was used. We discovered that a candidate answer with a high score given by Document QA and whose entity type is <em>organization</em> or something similar is considerably likely to be a correct dataset name. Due to this pattern we were able to create neural network classifier to filter out candidate answers.</p>
<p>The classifier has the following architecture:</p>
<ol type="1">
<li><p>Input size: 10332 (10331 labels from Ultra-Fine Entity Typing and the Document QA score)</p></li>
<li><p>1 hidden layer with 50 neurons</p></li>
<li><p>Output size: 2</p></li>
</ol>
<p>The training set consists of 25172 examples and test set of 6293 examples. Adam optimizer was used and cross entropy was used as loss function.</p>
<h2 id="research-fields-retrieval">Research Fields Retrieval</h2>
<p>Our approach to get the research fields is based on TF-IDF similarity with Wikipedia articles. First, a set of Wikipedia articles about different research fields using the library MediaWikifor Python was obtained. The list of research fields provided the Coleridge Initiative for the Rich Context Competition was used to crawl Wikipedia. This list has three levels of hierarchy as the example in the figure <a href="#fig:researchfieldshiearchy" data-reference-type="ref" data-reference="fig:researchfieldshiearchy">1</a>.</p>
<figure>
<embed src="%7Bfieldshierarchy2.png%7D" id="fig:researchfieldshiearchy" style="width:7cm" /><figcaption>Research fields hierarchy<span label="fig:researchfieldshiearchy"></span></figcaption>
</figure>
<p>The leaf nodes of that hierarchy were searched in Wikipedia to retrieve specific research fields instead of general ones. For example: we were aiming to retrieve <em>Neurosurgery</em> instead of <em>Medicine</em>.</p>
<p>Then, using Scikit-learn <span class="citation" data-cites="scikit-learn">[@scikit-learn]</span>, a TF-IDF matrix of all the publications and Wikipedia articles of research fields was computed and the research field and all its superior nodes in the hierarchy associated to the most similar article were returned along with the similarity in the range [0,1]. The overall architecture can be seen in figure <a href="#fig:researchfields" data-reference-type="ref" data-reference="fig:researchfields">2</a>.</p>
<figure>
<embed src="%7Bresearchfields2.png%7D" id="fig:researchfields" style="width:7.5cm" /><figcaption>Overall architecture for research fields retrieval<span label="fig:researchfields"></span></figcaption>
</figure>
<h2 id="research-methods-retrieval">Research Methods Retrieval</h2>
<p>For the research methods retrieval task, we modeled it as an named-entity recognition (NER) problem. Research methods are considered to be an named entity and because of this, they can be tagged as research method label (RS) instead of common NER labels such as: <em>location</em>, <em>people</em>, etc. Figure <a href="#fig:research_method" data-reference-type="ref" data-reference="fig:research_method">[fig:research_method]</a> shows the main architecture of the model proposed by <span class="citation" data-cites="lample2016neural">[@lample2016neural]</span> and used in this task.</p>
<figure>
<img src="bi-lstm.png" alt="BiLSTM-CRF architecture" style="width:7.5cm" /><figcaption>BiLSTM-CRF architecture</figcaption>
</figure>
<p><span id="fig:research_method" label="fig:research_method">[fig:research_method]</span></p>
<p>The representation of a word using the model is obtained considering its context. We have the assumption that research methods have dependencies and constraints with words that appear in their surrounding context. Therefore, the conditional random field <span class="citation" data-cites="lafferty2001conditional">[@lafferty2001conditional]</span> layer in this model is suitable for detecting research methods by jointly tagging the whole sentence, instead of independently tagging each word.</p>
<p>In this task, research method phrases which appeared in the training set were marked. Then, we represented the data in CoNLL 2003 format <span class="citation" data-cites="tjong2003introduction">[@tjong2003introduction]</span>, using IOB tag (Inside, Outside, Beginning). Every token is labeled as B-RS if the token is the beginning of a research method, I-RS if it is inside a research method but not the first token, or O if otherwise. We used this type of data to train the model which could detect research methods in publications.</p>
<h1 id="experiment">Experiment</h1>
<p>The Coleridge Initiative released a labeled training set of 5000 publications and a labeled dev set of 100 publications in the phase 1 of the Rich Context Competition. After the end of the phase 1, a labeled holdout set of 5000 publications of phase 1 and another unlabeled training set of 5000 publications of phase 2 was also released. For training, we use 5000 publications from phase 1 training set and another 5000 publications from phase 1 holdout set.</p>
<p>The experiment was performed using Docker<span class="citation" data-cites="anderson2015docker">[@anderson2015docker]</span> on an Amazon Web Services (AWS) T2 2xlarge instance which contains 8 virtual CPU cores and 32GB memory without GPU.</p>
<h1 id="results-and-discussions">Results and Discussions</h1>
<p>Due to the difficulty of performing a quantitative analysis on a not extensively labeled dataset, a qualitative analysis was made. Several random publications were chosen and manually labeled by us to check the quality of our model and discover the strong and weak points.</p>
<h2 id="datasets-retrieval-1">Datasets Retrieval</h2>
<p>To analyze the effects of the query generation module and entity typing module, we performed analyses on 100 phase 1 dev set with 3 different settings:</p>
<ol type="1">
<li><p>Document QA only</p></li>
<li><p>Document QA + query generation module</p></li>
<li><p>Document QA + query generation module + entity typing module</p></li>
</ol>
<h3 id="document-qa-only">Document QA only</h3>
<p>Figure <a href="#fig:docqaonly" data-reference-type="ref" data-reference="fig:docqaonly">3</a> shows the results from 3 publications of phase 1 dev set with Document QA only. Compared to the other settings, Document QA only setting retrieves answers (dataset mentions) with high quality. However, the number of retrieved answers is notably small. For example, the result from <em>153.txt</em> publication was empty as in figure <a href="#fig:docqaonly" data-reference-type="ref" data-reference="fig:docqaonly">3</a>. In fact, our model using this setting can retrieve only 260 answers (predictions) from 100 publications of phase 1 dev set.</p>
<figure>
<img src="phase1.png" alt="Results from Document QA only" id="fig:docqaonly" style="width:8cm" /><figcaption>Results from Document QA only<span label="fig:docqaonly"></span></figcaption>
</figure>
<p>These results with fewer answers were expected, due to the difficulty of defining general queries as explained in section <a href="#sec:questiongen" data-reference-type="ref" data-reference="sec:questiongen">3.1.2</a>. Without a query generation module, our query was not representative enough to retrieve various forms and types of the dataset mentions.</p>
<h3 id="document-qa-query-generation-module">Document QA + query generation module</h3>
<p>Figure <a href="#fig:docqaquery" data-reference-type="ref" data-reference="fig:docqaquery">4</a> shows the results from 3 publications of phase 1 dev set with Document QA and query generation module. Because of the latter, our dataset retrieval model could retrieve a large number of answers. For example, the result from <em>153.txt</em> publication contains a large number of answers with correct answers such as <em>financial services FDI data</em> or <em>Micro Batabase Direct investment</em>. Therefore, we believe that the query generation module improves recall of the entire dataset retrieval model. Actually, our model using this setting can retrieve more than 2,000 answers (predictions) from 100 publications of phase 1 dev set.</p>
<p>However, compared to the Document QA only setting, there is a considerable number of noise. For example, in figure <a href="#fig:docqaquery" data-reference-type="ref" data-reference="fig:docqaquery">4</a>, <em>empirical, Table 1, Section 4</em> and etc., are not dataset mentions.</p>
<figure>
<img src="phase1+querygen.png" alt="Results from Document QA + query generation module" id="fig:docqaquery" style="width:8cm" /><figcaption>Results from Document QA + query generation module<span label="fig:docqaquery"></span></figcaption>
</figure>
<p>We believed that the reason of these noises is the several query terms potentially retrieve wrong answers. For example, we have a query term <em>"study"</em> to retrieve dataset mentions such as <em>"ANES 1952 Time Series Study"</em>. However, this term can also retrieve noises such as <em>"empirical study"</em>. These kinds of query terms are still needed to retrieve various forms and types of dataset mentions, but clearly generate some noises.</p>
<h3 id="document-qa-query-generation-module-entity-typing-module">Document QA + query generation module + entity typing module</h3>
<p>Figure <a href="#fig:docqaqueryentity" data-reference-type="ref" data-reference="fig:docqaqueryentity">5</a> shows the results from 3 publications of phase 1 dev set with Document QA, query generation module, and entity typing module. Thanks to the entity typing module, we can see that most of noises from query generation module have disappeared. Although a few right answers such as <em>"FDI data"</em> was filtered out and a few wrong answers such as <em>"4.2.1 Micro Data"</em> was not, overall precision is adequately improved by entity typing module. In addition, our model in this setting could retrieve 526 answers (predictions) from 100 publications of phase 1 dev set.</p>
<figure>
<img src="phase1+querygen+entitytyping.png" alt="Results from Document QA + query generation module + entity typing module" id="fig:docqaqueryentity" style="width:8cm" /><figcaption>Results from Document QA + query generation module + entity typing module<span label="fig:docqaqueryentity"></span></figcaption>
</figure>
<h2 id="research-fields-retrieval-1">Research Fields Retrieval</h2>
<p>We randomly selected 20 publications from the training set of phase 1, since our model does not require any training. The model was able to correctly predict 11. The strongest point is that the model is able to predict research fields which are significantly specific such as <em>Home health nursing management</em>. Among the weak points of the model, it has problems when two research fields are similar or share subtopics. Moreover, sometimes it fails due to the fact that it tries to retrieve excessively specific fields while more general ones would be suitable.</p>
<h2 id="research-methods-retrieval-1">Research Methods Retrieval</h2>
<p>20 random publications were selected from the training set of phase 2 and labelled. Our result is not as expected. The model is able to find proper research methods for 12 publications out of 20. For example, the model detects one of the research methods appeared in publication with id 15359 which is <em>Factor analysis</em>. However, the results contain a notably amount of noise. For example, the document with id 10751, the model retrieves several wrong answers like: <em>Reviews describe</em>, <em>Composite materials</em>, <em>Detailed databases</em>, etc. After analyzing this result, we found that the dataset we use for training is not appropriate for this task. For example, <em>Reliability</em> and <em>Independent variables</em> are marked as research methods, but actually they are not.</p>
<h1 id="conclusions-1">Conclusions</h1>
<p>In this work we proposed three information retrieval models to mine information from scientific publications. One to retrieve the mentions of the datasets used, another one to retrieve the research field and the last one to retrieve the research methods. Our contributions are as follow. We modeled the dataset retrieval task as an QA problem. In order to solve this task, we proposed a query generation module and a filter using entity types. We also proposed to use Wikipedia articles to retrieve research fields from scientific publications. Finally, we proposed to model the research method retrieval task as an NER problem.</p>
<h1 id="future-work-1">Future Work</h1>
<p>This work is the very first step of the Coleridge Initiative to build an “Amazon.com” for data users and data producers. The next step is to construct a system that recommends datasets to researchers. We have a hypothesis that datasets depend on research fields and vice versa. For example, in the research field <em>Question Answering</em>, a subfield of <em>Natural Language Processing</em> and <em>Computer Science</em>, the most commonly used dataset is SQuAD <span class="citation" data-cites="rajpurkar2016squad">[@rajpurkar2016squad]</span>. Therefore, according to our hypothesis, two publications using SQuAD are presumably to be in the same field, <em>Question Answering</em>. Based on this hypothesis, we intend to build hierarchical clusters of publications with the same research field. This way, a cluster will have publications with the same research field and similar datasets. As an example, the QA cluster will have papers about QA and those papers will use similar datasets like SQuAD and TriviaQA <span class="citation" data-cites="joshi2017triviaqa">[@joshi2017triviaqa]</span>. With these clusters, the system will be able to recommend datasets to data users. For example, if a publication is in the <em>Question Answering</em> field, the proposed system would be able to recommend the authors SQuAD and TriviaQA. Moreover, it would be able to recommend to data producers fields with a lack of datasets.</p>
<p>In addition, we also need to improve the performance of the models we built. For example, since we used a pretrained model in Document QA we think we could not exploit the whole potential of this system, so we would like to train our own model using a training set of publications.</p>
<h1 id="introduction-3">Introduction</h1>
<p>Scientists and analysts often face the problem of finding interesting research datasets and identifying who else used the data, in which research fields, and how the data has been analyzed from a methodological perspective. To address these problems, the Coleridge Initiative organized the Rich Context Competition<a href="RCC">1</a>. The competition invited international research teams to develop text analysis and machine learning tools that can discover relationships between research datasets, methods, and fields in scientific literature. The competition took place between October 2018 and February 2019 and included two phases[2]. The first phase was open for all teams which have submitted a letter of intent. Teams are then provided with a corpus of social science publications to develop and train machine learning algorithms for automatic research dataset, methods and field detection and linking. More concretely, one major subtask consisted of linking dataset mentions to a given set of around 10,000 dataset descriptions from the ICPSR’s research data index.[3] Only the best four teams from the first phase are invited to the second phase of the competition and asked to discover research datasets, methods, and fields in a larger corpus of social science publications. All submitted algorithms have to be made publicly available as open source tools. With this document, we (team RCC-5) aim to fulfill another requirement, i.e., the documentation and summary of the developed approach including data pre-processing, algorithms, and software.</p>
<h2 id="general-approach-and-software-components">General Approach and Software Components</h2>
<p>One of the central tasks in the RCC is the extraction of dataset mentions from text. Nevertheless, we considered the methods and fields discovery equally important. To this end, we decided to follow a module-based approach and developed tools that can be used separately but also as parts of a data processing pipeline. Figure [figure:pipeline] shows an overview of the software modules developed for the RCC competition, including their dependencies. Here, the upper three modules (gray) describe the pre-processing steps (cf. Section [sec:prepro]). The lower four modules (blue) are used to generate the output in a pre-specified format. The pre-processing step consists of extracting metadata and pure text from PDF documents. The extraction itself is done using the Cermine Tool[4] which returns a Journal Article Tag Suite<a href="Jats">5</a> XML document. Then, in a second step, text, metadata and references are extracted. The output of the pre-processing is then used by the software modules responsible for tackling the individual sub-tasks, i.e., discovering research datasets (cf. Section [sec:dataset-extraction]), methods (cf. Section [section:research_method_extraction]) and fields (cf. Section [section:field_classification]). Section [sec:techdoc] provides the technical details of the modules, i.e., input, output, and how to run the modules.</p>
<h2 id="first-phase-feedback">First Phase Feedback</h2>
<p>After the first phase, each team received feedback from the organizers of the RCC. The feedback is twofold and consists of a quantitative and qualitative evaluation. Unfortunately, our team did not perform very well regarding precision and recall. In contrast to this, our approach has been found convincing regarding the quality of results. The qualitative feedback result from a random sample of ten documents that are given to four judges. Judges are then asked to manually extract dataset mentions and calculate the overlap between their dataset extractions and the output of our algorithm. Other factors that judges took into consideration are specificity, uniqueness and multiple occurrences of dataset mentions. As for the extraction of research methods and fields no ground truth has been provided, these tasks were evaluated against the judges’ expert knowledge. Similarly to the extraction of dataset mentions, specificity and uniqueness have been considered for these two tasks. The feedback our team received acknowledged the fact that no ground truth has been provided and our efforts regarding the extraction of research methods and fields.</p>
<h1 id="data-and-pre-processing">Data and Pre-processing</h1>
<p>This section describes the data provided by the organizers of the RCC, the external data sources we used as well as our pre-processing steps.</p>
<h2 id="the-rcc-corpus">The RCC Corpus</h2>
<p>For the first phase, the data provided by the organizers consisted of 5,000 publications. Additionally, a development fold of 100 plain text publications, their metadata, a list of datasets of interest (including all datasets that were explicitly referenced in the curated corpus) were given. The list of datasets should not be considered complete as there could be additional datasets mentioned in these publications. The organizers also provided examples of social science research methods and fields vocabularies in term of SAGE Publications research field and method vocabularies. In the second phase of the competition, an additional set of 5,000 publications from the social sciences has been provided.</p>
<h2 id="external-data-sources">External Data Sources</h2>
<p>For developing our algorithms, we also utilized two external data sources. For the discovery of research methods and fields, we resort to data from Social Science Open Access Repository[6] (SSOAR). SSOAR is maintained at GESIS – Leibniz Institute for the Social Sciences collects and archives literature of relevance to the social sciences. In SSOAR, full texts are indexed using controlled social science vocabulary (Thesaurus[7], Classification[8]) and are assigned rich metadata. SSOAR offers documents in various languages. The corpus of English language publications that can be used for purposes of the competition consists of a total of 13,175 documents. All SSOAR documents can be accessed through the OAI-PMH[9] interface. Another external source that we used for discovery of research methods is the ACL Anthology Reference Corpus (Bird et al., 2008). ACL ARC is a corpus of scholarly publications about computational linguistics. The corpus consists of a total of 22,878 articles.</p>
<h2 id="pre-processing">Pre-processing</h2>
<p>Although the organizers of the RCC, offered plain texts for the publication, we decided to build our own pre-process pipeline. The pipeline uses the Cermine Tool to extract information from PDF documents. The main benefit of using this tool is the structured metadata output including better disambiguation of sections and paragraphs in the publications. The output XML file uses the Journal Article Tag Suite[10]. For the competition, there are only two interesting elements of the Jats XML format, i.e., &lt;front&gt; and &lt;body&gt;. The &lt;front&gt; element contains the metadata of the publication, whereas the &lt;body&gt; contains the publication text. Another advantage of Cermine is that the hyphenation and segmentation of paragraphs are carried out automatically. As a last step of the pre-processing, we remove all linebreaks from the publication text and output a list of metadata fields and values as shown in Table [tab:example-paragraph] for each publication paragraph.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Example Text Field Data</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">publication_id</td>
<td style="text-align: left;">12744</td>
</tr>
<tr class="even">
<td style="text-align: left;">label</td>
<td style="text-align: left;">paragraph_text</td>
</tr>
<tr class="odd">
<td style="text-align: left;">text</td>
<td style="text-align: left;">A careful reading of text, word</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">for word, was …</td>
</tr>
<tr class="odd">
<td style="text-align: left;">section_title</td>
<td style="text-align: left;">Data Analysis</td>
</tr>
<tr class="even">
<td style="text-align: left;">annotations</td>
<td style="text-align: left;">[{’start’: 270, ’end’: 295,</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">’type’: ’bibref’, …</td>
</tr>
<tr class="even">
<td style="text-align: left;">section_nr</td>
<td style="text-align: left;">[3, 2]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">text_field_nr</td>
<td style="text-align: left;">31</td>
</tr>
<tr class="even">
<td style="text-align: left;">para_in_section</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
<p>[tab:example-paragraph]</p>
<h1 id="dataset-extraction">Dataset Extraction</h1>
<h2 id="task-description">Task Description</h2>
<p>In scientific literature, datasets are specified to indicate, e.g., the data on which a analysis is performed, a certain finding or a claim is based on. In this competition, we focus on (i) extracting and (ii) linking datasets mention from social science publications to a list of given dataset references. Identifying dataset mention in literature is a challenging problem due to the lack of an established style of citing datasets. Furthermore, in many research publication, a correct citation of datasets is entirely missing (Boland et al., 2012). The following two sentences exemplify the problem. <strong>Example 1</strong>: <em>P-values are reported for the one-tail paired t-test on </em>Allbus* (dataset mention) and <em>ISSP</em> (dataset mention).<em> <strong>Example 2</strong>: </em>We used <em>WHO data from 2001</em> (dataset mention) to estimate the spreading degree of AIDS in Uganda.* We treat the problem of detecting dataset mentions in full-text as a Named Entity Recognition (NER) task.</p>
<h4 id="formal-problem-definition">Formal problem definition</h4>
<p>Let <em>D</em> denote a set of existing datasets <em>d</em> and the knowledgebase <em>K</em> as a set of known dataset references <em>k</em>. Furthermore, each element of <em>K</em> is referencing an existing dataset <em>d</em>. The Named Entity Recognition and linking task is defined as (i) the identification of dataset mentions <em>m</em> in a sentence, where <em>m</em> references a dataset <em>d</em> and (ii) linking them, when possible, to one element in <em>K</em> (i.e., the reference dataset list given by the RCC).</p>
<h2 id="challenges">Challenges</h2>
<p>With our method, we focus on the extraction of dataset mentions in the body of the full-text of scientific publications. We recognize three types a dataset can be mentioned: (i) The full name of a dataset like ”National Health and Nutrition Examination Survey“, (ii) an abbreviation (”NHaNES“) or (iii) a vague reference, e.g.,”the monthly statistic“. By each of these varieties, the NER task faces particular challenges. For the first type, the used dataset name can vary in different publications. Where one publication cites the dataset with”National Health and Nutrition Examination Survey“ the other could use the words ”Health and Nutrition Survey“. In a case where abbreviations are used a disambiguation problem occurs, e.g., in”WHO data“. WHO may describe the World Health Organization or the White House Office. The biggest challenge is again the lack of a precise gold standard that can be used to train a classifier. In the following we describe how we have dealt with this lack of ground truth data.</p>
<h2 id="phase-one-approach">Phase one approach</h2>
<p>The challenge of missing ground truth data is the main problem to handle during this competition. To this end, supervised learning methods for dataset mentions extraction from text are not directly applicable. To overcome this limitation, we resort to the provided list of dataset mentions and publication pairs and re-annotate the particular sentences in the publication text. This re-annotation is then used to train Spacy’s neural network based NER model[11]. We created a holdout set of 1000 publications and a training set of size 4000. We train our model using publication paragraphs as training samples. In the training set, 0.45 percent of the paragraphs contained mentions. For each positive training example, we added a negative example that does not contain dataset mentions and is sampled at random. We used a batch size of 25 and a dropout rate of 0.4. The model was trained for 300 iterations.</p>
<h4 id="evaluation">Evaluation</h4>
<p>We evaluated our model with respect to four metrics: strict precision and recall, and partial precision and recall. While the former are standard evaluation metrics, the latter are their relaxed variants in which the degree to which dataset mentions have to match can vary. Consider the following example of a partial match: “National Health and Nutrition Examination Survey” is the extracted dataset mention whereas, “National Health and Nutrition Examination Survey (NHANES)” represents the true dataset mention.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Partial Precision</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr class="even">
<td style="text-align: left;">Partial Recall</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Strict Precision</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr class="even">
<td style="text-align: left;">Strict Recall</td>
<td style="text-align: center;">0.81</td>
</tr>
</tbody>
</table>
<p>[table:dataset-mention-eval]</p>
<p>Table [table:dataset-mention-eval] show the results of the dataset mention extraction on the holdout set. The model is able to achieve high strict precision and recall values. As expected, the results are even better for the partial version of the metrics. But, this version indicates that even if we are not able to exactly match the dataset mention in text, we can find the right context with very high precision at least.</p>
<h2 id="phase-two-approach">Phase two approach</h2>
<p>In the second phase of the competition additional 5,000 publications have been provided. We extended our approach to consider the list with dataset names supplied by the organizers and re-annotated the complete corpus of 15.000 publication in the same manner as in phase one to obtain training data. This time we split the data in 80% for training and 20% for test.</p>
<h4 id="evaluation-1">Evaluation</h4>
<p>We resort to the same evaluation metrics as in phase one. However, we calculate precision and recall on the full-text of the publication and not on the paragraphs as in the first phase. Table [table:dataset-mention-eval-phase-two] show the results achieved by our model. We observe a lower precision and recall values. Compared to phase one, there is also a smaller difference between the precision an recall values for the strict and partial version of the metrics.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Partial Precision</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr class="even">
<td style="text-align: left;">Partial Recall</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Strict Precision</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr class="even">
<td style="text-align: left;">Strict Recall</td>
<td style="text-align: center;">0.87</td>
</tr>
</tbody>
</table>
<p>[table:dataset-mention-eval-phase-two]</p>
<h1 id="research-method-extraction">Research Method Extraction</h1>
<h2 id="task-description-1">Task Description</h2>
<p>Inspired by a recent work of Nasar et al. (Nasar et al., 2018), we define a list of basic entity types that give key-insights into scholarly publications. We adapted the list of semantic entity types to the domain of the social sciences with a focus on <em>research methods</em>, but also including related entity types such as <em>Theory, Model, Measurement, Tool, Performance</em>. We suspect that the division into semantic types might be helpful to find <em>research methods</em>, because related semantic entities types might provide clues or might be directly related to the research method itself. For instance, in order to realize a certain research objective, an experiment is instrumented where a specific combination of <em>methods</em> is applied to a <em>data set</em> that might be intellectual or <em>software</em>, thus achieving a specific <em>performance</em> and result in that context. <strong>Example</strong>: <em>P-values</em> (measurement) are reported for the <em>one-tail paired t-test</em> (method) on <em>Allbus</em> (dataset) and <em>ISSP</em> (dataset).</p>
<h4 id="formal-problem-definition-1">Formal problem definition</h4>
<p>Let <em>E</em> denote a set of entities. The Named Entity Recognition and Linking task consists of (i) identifying entity mentions <em>m</em> in a sentence and, (ii) linking them, when possible, to a reference knowledge base <em>K</em> (i.e, the SAGE Thesaurus[12]) and (iii) assigning a type to the entity, e.g., <em>research method</em>, selected from a set of given types. Given a textual named entity mention <em>m</em> along with the unstructured text in which it appears, the goal is to produce a mapping from the mention <em>m</em> to its referent real world entity <em>e</em> in <em>K</em>.</p>
<h2 id="challenges-1">Challenges</h2>
<p>There are some major challenges that any named entity recognition, classification and linking system needs to handle. First, regarding NER, identifying the entities boundary is important, thus detecting the exact sequence span. Second, ambiguity errors might arise in classification. For instance,‘range’ might be a domain-specific term from the knowledge base or belong to the general domain vocabulary. This is a challenging task for which context information is required. In the literature, this relates to the problem of <strong>domain adaptation</strong> which includes fine-tuning to specific named entity classes[13]. With respect to entity linking, another challenge is detecting name variations, since entities can be referred to in many different ways. Semantically similar words, synonyms or related words, which might be lexically or syntactically different, are often not listed in the knowledge base (e.g., the lack of certain terms like ‘questioning’ but not ‘questionnaire’). This problem of automatically detecting these relationships is generally known as <strong>linking problem</strong>. Note that part of this problem also results from PDF-to-text conversion which is error-prone. Dealing with incomplete knowledge bases, i.e. <strong>handling of out of vocabulary (OOV) items</strong>, is also a major issue, since knowledge bases are often not exhaustive enough and do not cover specific terms or novel concepts from recent research. Last but not least, the combination of different semantic types gives a more coherent picture of a research article. We hypothesize that such information would be helpful and results in an insightful co-occurrence statistics, and provides additional detail directly related to entity resolution, and finally helps to assess the <strong>relevance of terms</strong> by means of a score.</p>
<h2 id="our-approach---overview">Our Approach - Overview</h2>
<p>Our context-aware framework builds on Stanford’s CoreNLP and Named Entity Recognition System[14]. The information extraction process follows the workflow depicted in Figure [figure:pipeline], using separate modules for pre-processing, classification, linking and term filtering.</p>
<p>We envision the task of finding entities in scientific publications as a sequence labeling problem, where each input word is classified as being of a dedicated semantic type or not. In order to handle entities related to our domain, we train a novel machine learning classifier with major semantic classes, using training material from the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). Apart from this, we follow a domain adaptation approach inspired by (Agerri and Rigau, 2016) and ingest semantic background knowledge extracted from external scientific corpora, in particular the ACL Anthology (Bird et al., 2008; Gildea et al., 2018). We perform entity linking by means of a new gazetteer-based SAGE dictionary of Social Research Methods (Lewis-Beck et al., 2003), thus putting a special emphasis on the social sciences. The linking component addresses the synonymy problem and matches an entity despite name variations such as spelling variations. Finally, term filtering is carried out based on a termhood and unithood, while scoring is achieved by calculating a relevance score based on TF-IDF (cf. Section [para:relscore]).</p>
<p>Our research experiments are based on the repository for the Social Sciences SSOAR as well as the train and test data of the Rich Context Competition corpus[15]. Our work extends previous work on this topic (cf. (Eckle-Kohler et al., 2013)) in various ways: First, we do not limit our study to abstracts, but use the entire fulltext. Second, we focus on a broader range of semantic classes, i.e. <em>Research Method</em>, <em>Research Theory</em>, <em>Research Tool</em> and <em>Research Measurement</em>, tackling also the problem of identifying novel entities.</p>
<p>[pipeline] <img src="figures/research-methods/pipeline.png" title="fig:" alt="Overview of the entity extraction pipeline" style="width:47.0%" /></p>
<h4 id="distributed-semantic-models">Distributed Semantic Models</h4>
<p>For domain adaptation, we integrate further background knowledge. We use vector embeddings of words trained on additional corpora and which serve as input features to the CRF model. Semantic representations of words are a successful extension of common features, resulting in higher NER performance (Turian et al., 2010) and can be trained offline.</p>
<p>In this work, the word vectors were learned from the scientific ACL ARC[16] using Gensim with the skip gram model (cf. (Mikolov et al., 2013)) and a pre-clustering algorithm[17]. A summary of the size of the unlabeled English data used for training word embeddings can be found in Table [tab:UnlabeledData].</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Corpus</th>
<th style="text-align: left;">Articles</th>
<th style="text-align: left;">Documents/Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">ACL Corpus</td>
<td style="text-align: left;">22,878</td>
<td style="text-align: left;">806,791/2.5 GB</td>
</tr>
</tbody>
</table>
<h4 id="features">Features</h4>
<p>The features incorporated into the linear chain CRF are shown in the Table [tab:features]. The features depend mainly on the observations and on pairs of adjacent labels, using a log-linear combination. However, since simple token level training of CRFs leads to poor performance, more effective text features such as word shape, orthographic, gazetteer, Part-Of-Speech (POS) tags, along with word clustering (see Section [subsec:dist-model]) have been used.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Type</strong></th>
<th style="text-align: center;"><strong>Features</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Token unigrams</strong></td>
<td style="text-align: center;"><em>w</em><sub><em>i</em> − 2</sub>, <em>w</em><sub><em>i</em> − 1</sub>, <em>w</em><sub><em>i</em></sub>, <em>w</em><sub><em>i</em> + 1</sub>, <em>w</em><sub><em>i</em> + 2</sub>, …</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>POS unigrams</strong></td>
<td style="text-align: center;"><em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>i</em> − 1</sub>, <em>p</em><sub><em>i</em> − 2</sub></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Shapes</strong></td>
<td style="text-align: center;">shape and capitalization</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>NE-Tag</strong></td>
<td style="text-align: center;"><em>t</em><sub><em>i</em> − 1</sub>, <em>t</em><sub><em>i</em> − 2</sub></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>WordPair</strong></td>
<td style="text-align: center;">(<em>p</em><sub><em>i</em></sub>, <em>w</em><sub><em>i</em></sub>, <em>c</em><sub><em>i</em></sub>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>WordTag</strong></td>
<td style="text-align: center;">(<em>w</em><sub><em>i</em></sub>, <em>c</em><sub><em>i</em></sub>)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Gazetteer</strong></td>
<td style="text-align: center;">SAGE gazetteer</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Distributional Model</strong></td>
<td style="text-align: center;">ACL Anthology model</td>
</tr>
</tbody>
</table>
<h4 id="knowledge-resources">Knowledge Resources</h4>
<p>We use the SAGE thesaurus which includes well-defined concepts, an explicit taxonomic hierarchy between concepts as well as labels that specify synonyms of the same concept. A portion of terms is unique to the social science domain (e.g., ‘dependent interviewing’), while others are drawn from related disciplines such as statistics (e.g., ‘conditional likelihood ratio test’)[18]. However, since the thesaurus is not exhaustive and covers only the top-level concepts related to social science methods, our aim was to extend it by automatically extracting further terms from domain-specific texts, in particular from the Social Science Open Access Repository. More concretely, we carried out the following steps to extend SAGE as an off-line step. For step 2 and 3, candidate terms have been extracted by our pipeline for the entire SSOAR corpus.</p>
<ol type="1">
<li><p>Assignment of semantic types to concepts (manual)</p></li>
<li><p>Extracting terms variants such as abbreviations, synonyms, related terms from SSOAR (semi-automatic)</p></li>
<li><p>Computation of Term and Document Frequency Scores for SSOAR (automatic)</p></li>
</ol>
<h4 id="extracting-term-variants-such-as-abbreviations-synonyms-and-related-terms">Extracting term variants such as abbreviations, synonyms, and related terms</h4>
<p>26.082 candidate terms have been recognized and classified by our pipeline and manually inspected to a) find synonyms and related words that could be linked to SAGE, and b) build a post-filter for incorrectly classified terms. Moreover, abbreviations have been extracted using the algorithm of Schwartz and Hearst (Schwartz and Hearst, 2003). This way, a Named Entity gazetteer could be built and will be used at run-time. It comprises 1,111 terms from SAGE and 447 terms from the Statistics glossary as well as 54 previously unseen terms detected by the model-based classifier.</p>
<h4 id="computation-of-term-and-document-frequency-scores">Computation of Term and Document Frequency Scores</h4>
<p>Term frequency statistics have been calculated off-line for the entire SSOAR corpus. The term frequency at corpus level will be used at run time to determine the term relevance at the document level by calculating the TF-IDF scores. The most relevant terms from SAGE are listed in Table [tab:SAGET].</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">[tab:SAGET] <strong>SAGE Term</strong></th>
<th style="text-align: left;"><strong>TF-IDF Score</strong></th>
<th style="text-align: left;"><strong>Semantic Class</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Fuzzy logic</td>
<td style="text-align: left;">591,29</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="even">
<td style="text-align: left;">arts-based research</td>
<td style="text-align: left;">547,21</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cognitive interviewing</td>
<td style="text-align: left;">521,13</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="even">
<td style="text-align: left;">QCA</td>
<td style="text-align: left;">463,13</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="odd">
<td style="text-align: left;">oral history</td>
<td style="text-align: left;">399,68</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="even">
<td style="text-align: left;">market research</td>
<td style="text-align: left;">345,37</td>
<td style="text-align: left;">Research Field</td>
</tr>
<tr class="odd">
<td style="text-align: left;">life events</td>
<td style="text-align: left;">186,61</td>
<td style="text-align: left;">Research Field</td>
</tr>
<tr class="even">
<td style="text-align: left;">Realism</td>
<td style="text-align: left;">314,34</td>
<td style="text-align: left;">Research Theory</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Marxism</td>
<td style="text-align: left;">206,77</td>
<td style="text-align: left;">Research Theory</td>
</tr>
<tr class="even">
<td style="text-align: left;">ATLAS.ti</td>
<td style="text-align: left;">544,51</td>
<td style="text-align: left;">Research Tool</td>
</tr>
<tr class="odd">
<td style="text-align: left;">GIS</td>
<td style="text-align: left;">486,01</td>
<td style="text-align: left;">Research Tool</td>
</tr>
<tr class="even">
<td style="text-align: left;">SPSS</td>
<td style="text-align: left;">136,52</td>
<td style="text-align: left;">Research Tool</td>
</tr>
</tbody>
</table>
<h4 id="definition-of-a-relevance-score">Definition of a Relevance Score</h4>
<p>Relevance of terminology is often assessed using the notion of <em>unithood</em>, i.e. ‘the degree of strength or stability of syntagmatic combinations of collections’, and <em>termhood</em>, i.e. ‘the degree that a linguistic unit is related to domain-specific concepts’ (Kageura and Umino, 1996). Regarding <em>unithood</em>, the NER model implicitly contains heuristics about legal POS tag sequences for candidate terms, consisting of at least one noun (NN), preceeded or followed by modifiers such as adjectives (JJ), participles (VB*) or cardinal numbers (CD), complemented by wordshape features.</p>
<p>In order to find out if the candidate term also fulfills the <em>termhood</em> requirement, domain-specific term frequency statistics have been computed on the SSOAR repository, and set in contrast to general domain vocabulary terms. It has to be noted that only a small portion of the social science terms is actually unique to the domain (e.g., ‘dependent interviewing’), while others might be drawn from related disciplines such as statistics (e.g., ‘conditional likelihood ratio test’).</p>
<h4 id="preliminary-results">Preliminary Results</h4>
<p>Our method has been tested on 100 fulltext papers from SSOAR and 10 documents from the Rich Context Competition (RCC), all randomly selected from hold out corpora. In our experiments on SSOAR Social Science publications, we compared results to the given metadata information. The main finding was that while most entities from the SAGE thesaurus could be extracted and linked reliably (e.g., ’Paired t-test’), they could not be easily mapped to the SSOAR metadata terms, which consist of only a few abstract classes (e.g., ’quantitative analysis’). Furthermore, our tool was tested by the RCC organizer, were the judges reviewed 10 random publications and generated qualitative scores for each document.</p>
<h2 id="conclusion-and-future-work">Conclusion and Future Work</h2>
<p>We plan to carry out a more detailed evaluation on fulltext scholarly publications and assess the impact of different features used in the ML model, including background resources such as embeddings and dictionaries.</p>
<h1 id="research-field-classification">Research Field Classification</h1>
<h2 id="task-description-2">Task Description</h2>
<p>The goal of this task is to identify the research fields covered in social science publications. The RCC data does not provide a gold standard —annotated training data— for that task. To this end, we decided to train a classifier using annotated data from SSOAR. In this way, our interpretation of the task is to select one or more labels from a given set of labels for each publication. This approach is known as a mulit-label classification. In our case, a label represents a research field.</p>
<h2 id="our-approach---overview-1">Our approach - Overview</h2>
<p>Due to the unequal distribution of labels in the dataset, we need to guaranty enough training data for each label. We selected only labels with frequency over 300 for training the model which results in a total of 44 labels representing research fields. We decided to train a classification model based on the fasttext framework (Joulin et al., 2017). To train our model we resort to the abstracts of the publication, as this approach worked better than using the full-texts.</p>
<h2 id="evaluation-2">Evaluation</h2>
<p>Figure [fig:results_fasttext] shows the performance of the model regarding various evaluation metrics for different thresholds. A label is assigned to a publication if the model outputs a probability for the label above the defined threshold. In multi-label classification, this allows us to evaluate our model from different perspectives.</p>
<p><img src="figures/research-fields/fast-text-evaluation.png" alt="Precision-Recall vs. Threshold" style="width:49.0%" /></p>
<h1 id="technical-documentation">Technical Documentation</h1>
<p>The project contains the following modules listed in the order in which they are excecuted.</p>
<h2 id="pre-processing-1">Pre-processing</h2>
<h3 id="pdf-text-extraction">PDF Text Extraction</h3>
<p><strong>Module name :</strong></p>
<p>Cermine_NlmJat_extractor <strong>Function:</strong></p>
<p>Converts each PDF files of a given folder to JATS XML Format. Each input PDF File is transformed to one XML File. <strong>Bash function call:</strong></p>
<pre><code>java -jar target/cermineXMLextraction-1.0.0-jar-with-dependencies.jar</code></pre>
<p><strong>Parameter (2):</strong></p>
<pre><code>-s &lt;source folder&gt;\
-t &lt;target folder&gt;</code></pre>
<p><strong>Returns:</strong></p>
<p>XML Files in JATS XML Format. <strong>Build:</strong></p>
<p>This is a Java program using Maven build tool. <strong>build call:</strong></p>
<p>mvn install</p>
<h3 id="extraction-of-text-from-jats-xml">Extraction of text from JATS XML</h3>
<p><strong>Module name :</strong></p>
<p>preprocess-rcc-data <strong>Function:</strong></p>
<p>Transform text from JATX XML Format into a JSON File containing a list of textfields with essential metadata for each JATS XML file of a given folder. <strong>Bash function call:</strong></p>
<pre><code>`python3  ./jats_text_extractor.py `</code></pre>
<p><strong>Parameter (4):</strong></p>
<pre><code>&lt;source folder&gt;
&lt;target folder&gt;
&lt;limiting number of files to transform (-1: all)&gt; 
&lt;number of cores to use for multiprocessing (-1: all)&gt;</code></pre>
<p><strong>Returns:</strong></p>
<p>A JSON File for each given XML File in the source folder</p>
<h3 id="metadata-extraction">Metadata Extraction</h3>
<p><strong>Module name :</strong></p>
<p>preprocess-rcc-data <strong>Function:</strong></p>
<p>Extracts structured metadata and references from all JATS XML files in a given folder into two Files. One containing the metadata from all Publications in JATS XML files and one containing all references from the JATS XML Files. The target file format is JSON. <strong>Bash function call:</strong></p>
<pre><code>python3  ./jats_metadata_extractor.py</code></pre>
<p><strong>Parameter (3):</strong></p>
<pre><code>&lt;source folder&gt;
&lt;target filename for metadata&gt;
&lt;target filename for references&gt;</code></pre>
<p><strong>Returns:</strong></p>
<p>Two JSON files containing metadata and references from all XML Files</p>
<h2 id="dataset-mentions-1">Dataset Mentions</h2>
<h3 id="dataset-mention-extraction">Dataset Mention Extraction</h3>
<p><strong>Module:</strong></p>
<p>dataset-mention-extraction <strong>Function:</strong></p>
<p>Extract dataset mentions from all JSON Files from a given folder with a given spacy model. <strong>Bash function call:</strong></p>
<pre><code>python3  ./predict_mentions.py</code></pre>
<p><strong>Parameter (4):</strong></p>
<pre><code>&lt;source folder&gt;
&lt;name of spacy model folder&gt;
&lt;target filename rcc-output&gt;
&lt;target filename internal format&gt;</code></pre>
<p><strong>Returns:</strong></p>
<p>Two JSON files containing the found dataset mentions in all given JSON Files. One in RCC defined output. One in Internal format including the senctence the dataset mention occures. <strong>Train:</strong></p>
<p>For training we submit a jupyter notebook with all needed code. <em>Train_spacy_ner_prod.ipynb</em> <strong>Build (For training only):</strong></p>
<p>Install english spacy language model. This can be done with ‘python -m spacy download en‘.</p>
<h3 id="dataset-linking-only-phase-1">Dataset Linking (only Phase 1)</h3>
<p><strong>Module:</strong></p>
<p>dataset-prediction <strong>Function:</strong></p>
<p>Links dataset mentions given a JSON file in internal format to datasets listed in a given JSON File. <strong>Bash function call:</strong></p>
<pre><code>python3  ./retrieve.py</code></pre>
<p><strong>Parameter (3):</strong></p>
<pre><code>&lt;JSON filename of extracted mentions&gt;
&lt;JSON filename of dataset list to match&gt;
&lt;output filename for dataset citations&gt;</code></pre>
<p><strong>Returns:</strong></p>
<p>JSON file in the format defined by the competition containing information about links between publications and datasets.</p>
<h2 id="research-method-extraction-1">Research Method Extraction</h2>
<p><strong>Module name :</strong></p>
<p>research-method-extractor <strong>Function:</strong></p>
<p>Extracts research method terms from JSON files with text information from publications. <strong>Bash function call:</strong></p>
<pre><code>java -jar target/gesisents-0.1-jar-with-dependencies.jar</code></pre>
<p><strong>Parameter (3):</strong></p>
<pre><code>&lt;source folder&gt;
&lt;target file name&gt;
&lt;Limit to reduce the number of processed files (-1:all)&gt;</code></pre>
<p><strong>Returns:</strong></p>
<p>A JSON file in the format defined by the competition containing information about publications and research methods. <strong>Build:</strong></p>
<p>This is a Java program using Maven build tool. <strong>build call:</strong></p>
<p>mvn install</p>
<h2 id="research-field-classifier">Research Field Classifier</h2>
<p><strong>Module name :</strong></p>
<p>research-field-detector <strong>Function:</strong></p>
<p>Classifies given abstracts with classoz Labels <strong>Bash function call:</strong></p>
<pre><code>python3  ./fasttext_predictor.py</code></pre>
<p><strong>Parameter (4):</strong></p>
<pre><code>&lt;filename of JSON file with abstracts&gt;
&lt;filename of fasttext model&gt;
&lt;filename of label dictionary in JSON&gt;
&lt;target filename labels in &gt;</code></pre>
<p><strong>Returns:</strong></p>
<p>A JSON file in the format defined by the competition containing information about publications and research fields.</p>
<h1 id="acknowledgments-1">Acknowledgments</h1>
<p>We would like to thank GESIS for giving us the time and resources to participate in the competition.</p>
<p>Agerri R and Rigau G (2016) Robust multilingual named entity recognition with shallow semi-supervised features. <em>Artificial Intelligence</em> 238. Elsevier: 63–82.</p>
<p>Bird S, Dale R, Dorr BJ, et al. (2008) The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. In: <em>Proceedings of the sixth international conference on language resources and evaluation (lrec 2008)</em>, 2008. European Language Resources Association (ELRA).</p>
<p>Boland K, Ritze D, Eckert K, et al. (2012) Identifying references to datasets in publications. In: <em>International conference on theory and practice of digital libraries</em>, 2012, pp. 150–161. Springer.</p>
<p>Eckle-Kohler J, Nghiem T-D and Gurevych I (2013) Automatically assigning research methods to journal articles in the domain of social sciences. In: <em>Proceedings of the 76th asis&amp;T annual meeting: Beyond the cloud: Rethinking information boundaries</em>, 2013, p. 44. American Society for Information Science.</p>
<p>Finkel JR, Grenager T and Manning C (2005) Incorporating non-local information into information extraction systems by gibbs sampling. In: <em>Proceedings of the 43rd annual meeting on association for computational linguistics</em>, 2005, pp. 363–370. Association for Computational Linguistics.</p>
<p>Gildea D, Kan M-Y, Madnani N, et al. (2018) The acl anthology: Current state and future directions. In: <em>Proceedings of workshop for nlp open source software (nlp-oss)</em>, 2018, pp. 23–28.</p>
<p>Joulin A, Grave E, Bojanowski P, et al. (2017) Bag of tricks for efficient text classification. In: <em>Proceedings of the 15th conference of the european chapter of the association for computational linguistics: Volume 2, short papers</em>, April 2017, pp. 427–431. Association for Computational Linguistics.</p>
<p>Kageura K and Umino B (1996) Methods of automatic term recognition: A review. <em>Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication</em> 3(2). John Benjamins Publishing Company: 259–289.</p>
<p>Lewis-Beck M, Bryman AE and Liao TF (2003) <em>The Sage Encyclopedia of Social Science Research Methods</em>. Sage Publications.</p>
<p>Mikolov T, Sutskever I, Chen K, et al. (2013) Distributed representations of words and phrases and their compositionality. In: <em>Advances in neural information processing systems</em>, 2013, pp. 3111–3119.</p>
<p>Nasar Z, Jaffry SW and Malik MK (2018) Information extraction from scientific articles: A survey. <em>Scientometrics</em> 117(3). Springer: 1931–1990.</p>
<p>QasemiZadeh B and Schumann A-K (2016) The acl rd-tec 2.0: A language resource for evaluating term extraction and entity recognition methods. In: <em>LREC</em>, 2016.</p>
<p>Schwartz AS and Hearst MA (2003) A simple algorithm for identifying abbreviation definitions in biomedical text. In: <em>Pacific symposium on biocomputing</em>, 2003, pp. 451–462.</p>
<p>Turian J, Ratinov L and Bengio Y (2010) Word representations: A simple and general method for semi-supervised learning. In: <em>Proceedings of the 48th annual meeting of the association for computational linguistics</em>, Stroudsburg, PA, USA, 2010, pp. 384–394. ACL ’10. Association for Computational Linguistics. Available at: <a href="http://dl.acm.org/citation.cfm?id=1858681.1858721" class="uri">http://dl.acm.org/citation.cfm?id=1858681.1858721</a>.</p>
<p>[1] <a href="https://coleridgeinitiative.org/richcontextcompetition" class="uri">https://coleridgeinitiative.org/richcontextcompetition</a></p>
<p>[2] <a href="https://coleridgeinitiative.org/richcontextcompetition#competitionschedule" class="uri">https://coleridgeinitiative.org/richcontextcompetition#competitionschedule</a></p>
<p>[3] <a href="https://www.icpsr.umich.edu/index.html" class="uri">https://www.icpsr.umich.edu/index.html</a></p>
<p>[4] <a href="https://github.com/CeON/CERMINE" class="uri">https://github.com/CeON/CERMINE</a></p>
<p>[5] <a href="https://jats.nlm.nih.gov" class="uri">https://jats.nlm.nih.gov</a></p>
<p>[6] <a href="https://www.gesis.org/ssoar/home" class="uri">https://www.gesis.org/ssoar/home</a></p>
<p>[7] <a href="https://www.gesis.org/en/services/research/tools/thesaurus-for-the-social-sciences" class="uri">https://www.gesis.org/en/services/research/tools/thesaurus-for-the-social-sciences</a></p>
<p>[8] <a href="https://www.gesis.org/angebot/recherchieren/tools-zur-recherche/klassifikation-sozialwissenschaften" class="uri">https://www.gesis.org/angebot/recherchieren/tools-zur-recherche/klassifikation-sozialwissenschaften</a> (in German)</p>
<p>[9] <span><a href="http://www.openarchives.org" class="uri">http://www.openarchives.org</a></span></p>
<p>[10] <a href="https://jats.nlm.nih.gov" class="uri">https://jats.nlm.nih.gov</a></p>
<p>[11] <a href="spacy.io">spacy.io</a></p>
<p>[12] http://methods.sagepub.com</p>
<p>[13] apart from those used in traditional NER systems like <em>Person</em>, <em>Location</em>, or <em>Organization</em> with abundant training data, as covered in the Stanford NER system(Finkel et al., 2005)</p>
<p>[14] <a href="https://nlp.stanford.edu/projects/project-ner.shtml" class="uri">https://nlp.stanford.edu/projects/project-ner.shtml</a></p>
<p>[15] <a href="https://coleridgeinitiative.org/richcontextcompetition" class="uri">https://coleridgeinitiative.org/richcontextcompetition</a> with a total of 5,000 English documents</p>
<p>[16] <a href="https://acl-arc.comp.nus.edu.sg/" class="uri">https://acl-arc.comp.nus.edu.sg/</a></p>
<p>[17] Word embeddings are trained with a skip gram model using embedding size equal to 100, word window equal to 5, minimal occurrences of a word to be considered 10. Word embeddings are clustered using agglomerative clustering with a number of clusters set to <span>500,600,700</span> Ward linkage with euclidean distance is used to minimize the variance within the clusters.</p>
<p>[18] A glossary of statistical terms as provided in <a href="https://www.statistics.com/resources/glossary/" class="uri">https://www.statistics.com/resources/glossary/</a> has been added as well.</p>
<h1 id="introduction-4">Introduction</h1>
<h2 id="rich-context-competition">Rich Context Competition</h2>
<p>The goal of the Rich Context Competition<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, organized by the New York University under their Coleridge Initiative, was to automate the discovery of research datasets, associated research methods and fields in research publications belonging to the domain of Social Sciences. It was carried out in two phases. In the first phase (Phase-1), we were provided with a list of datasets along with their metadata (dataset vocabulary), a training corpus of 5000 publications containing publication metadata (2500 of them were labeled) and an additional dev fold of 100 publications. Apart from this, we were also given Social Science Methods and Fields vocabularies by SAGE Publications<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. To carry out Phase-1 evaluation, a separate corpus of 5000 labeled publications was held back.</p>
<p>In the second phase (Phase-2), in addition to the Phase-1 data for training, we were provided with the Phase-1 holdout set consisting of 5000 labeled publications and an additional corpus of 5000 unlabeled publications. The evaluation of the second phase was carried out by the organizers on another corpus that contained 5000 unlabeled publications. Note that the labeled data in both the phases was for Dataset Detection only. There was no ground truth for Research Methods and Fields.</p>
<h2 id="project-architecture">Project Architecture</h2>
<figure>
<embed src="images/flowchart_paper.pdf" id="fig:flowchart" style="width:99.0%" /><figcaption>Data Flow Pipeline (Red lines depict the flow of given and generated files between components whereas black lines represent the generation of final output files)<span label="fig:flowchart"></span></figcaption>
</figure>
<p>Our pipeline (shown in Fig. <a href="#fig:flowchart" data-reference-type="ref" data-reference="fig:flowchart">1</a>) consisted of three main components: 1) Preprocessing, 2) Fields and Methods Identification and 3) Dataset Extraction. The Preprocessing module read the text from publications and generated some additional files (see Section <a href="#preprocess" data-reference-type="ref" data-reference="preprocess">2</a> for details). These files along with the given Fields and Methods vocabularies were used to infer Research Fields and Methods from the publications. Then, the information regarding fields was passed onto the Dataset Detection module and using the Dataset Vocabulary, it identified Dataset Citations and Mentions. The following sections provide a detailed overview of each of these components.</p>
<h1 id="preprocess">Preprocessing</h1>
<p>The publications were provided to us in two formats: PDF and text. For Phase-1, we used the given text files, however during Phase-2, we came across many articles in the training files that had not been properly converted to text and contained mostly non-ASCII characters. In order to work with such articles, we relied on the open source tool <code>pdf2text</code> from <code>poppler suite</code><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> to extract text from PDFs.<br />
</p>
<p>Once we had the text files, we followed the rule-based approach as proposed in <span class="citation" data-cites="DBLP:journals/ploscb/WestergaardSTJB18">[@DBLP:journals/ploscb/WestergaardSTJB18]</span> for pre-processing. The following series of operations based mostly on regular expressions were performed:</p>
<ul>
<li><p>Words split by hyphens were de-hyphenated</p></li>
<li><p>Irrelevant data was removed (i.e., equations, tables, acknowledgment, references);</p></li>
<li><p>Main sections (i.e., abstract, keywords, JEL-Classification, methodology/data, summary, conclusion) were identified and extracted;</p></li>
<li><p>Noun phrases from these sections were extracted (using the python library, spaCy<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>).</p></li>
</ul>
<p>If a section was not found in the article (because of no explicit mention), then only the sections that could be detected were extracted. The remaining content was saved as ‘reduced_content’ after cleaning to prevent loss of any meaningful data.</p>
<p>Another open source tool that we used was <code>pdfinfo</code> from <code>poppler suite</code> to extract PDF metadata that very often contained the keywords and subject of an article. This tool was helpful in those cases where the keywords were not found by the regular expression.<br />
In the end, the preprocessing module generated four text files for a publication: PDF-converted text, PDF-metadata, processed articles containing relevant data, and noun phrases from the relevant sections, respectively. These files were then passed on to the other two components of the pipeline, which have been discussed below.</p>
<h1 id="approach">Approach</h1>
<h2 id="research-fields-and-methods-identification">Research Fields and Methods Identification</h2>
<h3 id="vocabulary-generation-and-model-preperation">Vocabulary Generation and Model Preperation</h3>
<ol type="1">
<li><p><strong>Research Methods Vocabulary</strong>: In Phase-1 of the challenge, we used the given methods vocabulary. However, in Phase-2, based on the evaluation feedback, we created our own Research Methods Vocabulary using Wikipedia and DBpedia.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> We manually curated a list of all the relevant statistical methods from Wikipedia<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> and fetched their descriptions from the corresponding DBpedia resources. For each label in the vocabulary, we extracted noun phrases from its description and added them to the vocabulary.</p></li>
<li><p><strong>Research Fields Vocabulary</strong>: For both the phases, we used the given research fields vocabulary and, just like the methods vocabulary, added noun phrases from the description of the labels to it. In addition, we created a blacklist of terms that didn’t contain any domain-specific information, such as; Mixed Methods, Meta Analysis, Narrative Analysis and the like for Phase-2.</p></li>
<li><p><strong>Word2Vec Model generation</strong>: In this pre-processing step, we used the above-mentioned vocabulary files to generate a vector model for both research fields and methods. The vector model was generated by using the labels and description of the available research fields and methods and then using the noun phrases present in them to form a sum vector. The sum vector was basically the sum of all the vectors of the words present in a particular noun phrase. A pre-trained Word2Vec model <span class="citation" data-cites="DBLP:journals/corr/abs-1301-3781">[@DBLP:journals/corr/abs-1301-3781]</span> was used to extract the vectors of the individual words.</p></li>
<li><p><strong>Research Method training results creation</strong>: For research methods, we generated an intermediate result file for the publications present in the training data. It was generated using a <code>naïve finder algorithm</code> that, for each publication, selected the research method with the highest cosine similarity to any of its noun phrase’s vectors. This file was later used to assign weights to research methods using Inverse Document Frequency.</p></li>
</ol>
<h3 id="processing-with-trained-models">Processing with Trained Models</h3>
<ul>
<li><p><strong>Finding Research Fields and Methods:</strong> To find the research fields and methods for a given list of publications, we performed the following steps: (At first, Step 1 was executed for all the publications, thereafter Step 2 and 3 were executed iteratively for each publication).</p>
<ol type="1">
<li><p><strong>Naïve Research Method Finder run</strong> - In this step, we executed the <code>naïve research method finding algorithm</code> against all the current publications and then merged the results with the existing result from the <code>research methods’ preprocessing step</code>. The combined result was then used to generate IDF weight values for each <code>research method</code>, in order to compute the significance of recurring terms.</p></li>
<li><p><strong>IDF-based Research Method Selection</strong> - We re-ran the algorithm to find the closest research method to each noun phrase and then sorted the pairs based on their weighted cosine similarity. The weights were taken from the IDF values generated in the first step and the manual weights assigned (section-wise weightage). Here, the noun phrases that came from the methodology section and from the methods listed in JEL-classification (if present) were given a higher preference. The pair with the highest weighted cosine similarity was then chosen as the Research Method of the article.</p></li>
<li><p><strong>Research Field Finder run</strong> - In this step, we first found the closest research field from each noun phrase in the publication. Then we selected the Top N (= 10) pairs that had the highest weighted cosine similarity. Afterwards, the noun phrases that had a similarity score less than a given threshold (= 0.9) were filtered out. The end-result was then passed on to the post-processing algorithm.<br />
For weighted cosine similarity, the weights were assigned manually based on the section of publication from which the noun phrases came. In general, noun phrases from title and keywords were given a higher preference than other sections.</p></li>
<li><p><strong>Research Field Selection</strong> - The top-ranked term from the result of step 3, which was not present in the blacklist of irrelevant terms, was marked as the research field of the article.</p></li>
</ol></li>
</ul>
<h2 id="dataset-extraction-1">Dataset Extraction</h2>
<p>For identifying the datasets in a publication, we followed two approaches and later combined results from both. Both the approaches have been described below.</p>
<ol type="1">
<li><p><strong>Simple Dataset Mention Search:</strong> We chose the dataset citations from the given Dataset Vocabulary that occurred for one dataset only and used these unique mentions to search for the corresponding datasets in the text documents. Then, we computed a frequency distribution of the datasets. As can be seen from Fig. <a href="#fig:graph" data-reference-type="ref" data-reference="fig:graph">2</a>, certain dataset mentions occurred more often than others, which increased the number of false positives. Therefore, we filtered out those mentions that occured more than a certain threshold value (=1.20) multiplied by the median of the frequency distribution and passed the remaining mentions to an interim result file.</p>
<figure>
<embed src="images/freq.pdf" id="fig:graph" /><figcaption>Frequency Distribution of Dataset Citations<span label="fig:graph"></span></figcaption>
</figure></li>
<li><p><strong>Rasa-based Dataset Detection:</strong> In our second approach, we trained an entity extraction model based on conditional random fields using Rasa NLU <span class="citation" data-cites="DBLP:journals/corr/abs-1712-05181">[@DBLP:journals/corr/abs-1712-05181]</span>. We particularly tested two configurations for training the CRF-based NER model. In Phase-1, the 2500 labeled publications from the training dataset were used for training the Rasa NLU<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> model. Later in Phase-2, when the Phase-1 holdout corpus was released, we combined its 5000 labeled publications with the previously given 2500 labeled publications and then retrained the model again with these 7500 labeled publications.<br />
<strong>Running the CRF-Model:</strong> The trained model was run against the preprocessed data to detect dataset citations and mentions. Only the entities that had a confidence score greater than a certain threshold value (=0.72) were considered as dataset mentions. A dataset mention was considered as a citation only if it was found in the given Dataset Vocabulary and if it belonged to the research field of the article. To check if a dataset belonged to the field of research, we found the cosine similarity of the terms in the ‘subjects’ field of the Dataset Vocabulary with the keywords, and identified the Research Field of the article.</p></li>
<li><p><strong>Combining the two approaches:</strong> The output generated by the rasa-based approach was first checked for irrelevant citations before a union was performed to combine the results. This was done by checking if a given dataset_id occured more than a threshold value (=1.20) multiplied by the median of the frequency distribution (same as the filtering process of the Simple Dataset Mention Search).</p></li>
</ol>
<p>Note that, the threshold values mentioned above were set after some experiments of trial and testing. For dataset extraction, the goal was to keep the number of false positives low while not compromising the true positives. For research methods and fields, a manual evaluation (see the next section for details) was done to test if the results made sense with the articles.</p>
<h1 id="evaluation-3">Evaluation</h1>
<p>We performed a quantitative evaluation for Dataset Extraction using the evaluation script provided by the competition organizers. This evaluation (see Table <a href="#tab:dataset" data-reference-type="ref" data-reference="tab:dataset">[tab:dataset]</a>) was carried out against the validation data, wherein we compared four different configurations. As can be inferred from the table, there was only a slight increase in performance for the Rasa-based model, when the training samples were increased. However, combining it with the Simple Dataset Mention Search, increased the performance by <em>19.42%</em>. Interestingly, there was no improvement in performance in the combined approach even when the training samples for Rasa Model were increased. This might be because of the removal of frequently-occuring terms from the Rasa-generated output, based on the frequency distribution of dataset mentions as computed in the Simple Dataset Mention Search.<br />
</p>
<p>M2.2cm | M2.3cm | M2.2cm M2.2cm M2.2cm &amp; &amp;<br />
<strong>Metrics</strong>&amp; <strong>Rasa-based Approach</strong> (2500) &amp; <strong>Rasa-based Approach</strong> (7500) &amp; <strong>Combined Approach</strong> (2500) &amp; <strong>Combined Approach</strong> (7500)<br />
<strong>Precision</strong> &amp; 0.382 &amp; 0.388 &amp; <strong>0.456</strong> &amp; <strong>0.456</strong><br />
<strong>Recall</strong> &amp; 0.26 &amp; 0.26 &amp; <strong>0.31</strong> &amp; <strong>0.31</strong><br />
<strong>F1</strong> &amp; 0.309 &amp; 0.311 &amp; <strong>0.369</strong> &amp; <strong>0.369</strong><br />
</p>
<p>For Research Fields and Methods, we carried out a qualitative evaluation against 10 randomly selected articles from Phase-1 holdout corpus. Tables <a href="#tab:field" data-reference-type="ref" data-reference="tab:field">[tab:field]</a> and <a href="#tab:method" data-reference-type="ref" data-reference="tab:method">[tab:method]</a> depict a comparison between the predicted fields and methods in Phase-1 and Phase-2. In general, our models returned a more granular output in the second phase, solely because of the modifications we made in the vocabularies.</p>
<p>C1cm C4.5cm C3cm C3.5cm <strong>pubid</strong> &amp; <strong>Keywords</strong> &amp; <strong>Phase-1</strong> &amp; <strong>Phase-2</strong><br />
10328 &amp; Cycling for transport, leisure and sport cyclists &amp; Health evaluation &amp; <strong>Public health and health promotion</strong><br />
7270 &amp; Older adult drug users, harm reduction &amp; Health Education &amp; <strong>Correctional health care</strong><br />
6053 &amp; Economic conditions - crime relationship, homicide &amp; Homicide &amp; <strong>Gangs and crime</strong><br />
</p>
<p>C1cm C4.5cm C3cm C3.5cm <strong>pubid</strong> &amp; <strong>Keywords</strong> &amp; <strong>Phase-1</strong> &amp; <strong>Phase-2</strong><br />
10328 &amp; Thematic content analysis &amp; Thematic analysis &amp; <strong>Sidak correction</strong><br />
7270 &amp; Interviews conducted face to face, finding systematic patterns or relationships among categories identified by reading the interview transcript &amp; Qualitative interviewing &amp; <strong>Sampling design</strong><br />
6053 &amp; Autoregressive integrated moving average (ARIMA) time-series model &amp; Methodological pluralism &amp; <strong>Multivariate statistics</strong><br />
</p>
<h1 id="discussion">Discussion</h1>
<p>Throughout the course of this competition, we encountered several challenges and limitations in all the three stages of the pipeline. In the preprocessing step, the appropriate extraction of text from PDFs turned out to be rather challenging. This was especially due to the varied formats of the publications, which made the extraction of specific sections—that contained all data relevant to our work—demanding. As mentioned before, if there was no explicit mention of the key-terms like <code>Abstract, Keywords, Introduction, Methodology/Data, Summary, Conclusion</code> in the text, then the content was saved as ‘reduced_content’ after applying all other preprocessing steps and filtering out any irrelevant data.<br />
Our experiments suggest that the labeled publications we received for dataset detection were not uniform in the dataset mentions provided, which made it difficult to train an entity extraction model even with an increased number of training samples. Hence, there was only a slight improvement in performance when the Rasa-model was trained with 7500 publications instead of 2500. This was also why we combined the Rasa-based approach with the Simple Dataset Mention Search, so that at least the datasets that were present in the vocabulary didn’t get missed.</p>
<p>Regarding the fields and methods, vocabularies played an immense role in their identification. The vocabularies that were provided by the SAGE publications contained some terms that were either polysemous or very high-level and therefore, were picked up by our model very often. Hence, for research methods, we created our own vocabulary containing all the relevant statistical methods, and for fields, we introduced a blacklist of irrelevant terms and looked it up each time, before writing the result to the output file. Since the focus was on more granulated results, we tried to look for open ontologies for Social Science Fields and Methods and unfortunately, could not find any. It is worth mentioning that since our approach for Fields and Methods identification relied heavily upon vocabularies, it could not find any new methods or fields from the publications.</p>
<h1 id="future-agenda">Future Agenda</h1>
<p>The data provided to us in the competition displayed a cornucopia of inconsistencies even after human processing. We hence propose that machine-aided methods for computing correct and complete structured representation of publications are of central importance for scientific research. Previous works on never-ending learning have shown how humans and extraction algorithms can work together to achieve high-precision and high-recall knowledge extraction from unstructured sources. In our future work, we hence aim to populate <strong>scientific knowledge graphs</strong> based on never-ending learning. The methodology we plan to develop will be domain-independent and rely on active learning to classify, extract, link and publish scientific research artifacts extracted from open-access papers. The resulting graphs will</p>
<ul>
<li><p>rely on advanced distributed storage for RDF to scale to the large number of publications available;</p></li>
<li><p>be self-feeding, i.e., crawl the web for potentially relevant content and make this content available for processing;</p></li>
<li><p>be self-repairing, i.e., be able to update previous extraction results based on insights gathered from new content;</p></li>
<li><p>be weakly supervised by humans, who would assist in correcting wrong hypotheses;</p></li>
<li><p>provide standardized access via W3C Standards such as SPARQL.</p></li>
</ul>
<p>Having such knowledge graphs would make it easier for the researchers (both young and veteran) to easily follow along with their domain of fast-paced research and eliminate the need to manually update the domain-specific ontologies for fields, methods and other metadata as new research innovations come up.</p>
<h1 id="abstract-1">Abstract</h1>
<p>With the vast number of datasets and literature collections available for research today, it is very difficult to keep track on the use of datasets and literature articles for scientific research and discovery. Many datasets and research work using them are left undiscovered and under-utilized due to the lack of available search tools to automatically find out who worked with the data, on what research topics, using what research methods and generating what results. The Coleridge Rich Context Competition (RCC) therefore aims to build automated dataset discovery tools for analysing and searching social science research publications. In this paper, we describe our approach to solving the first phase of Coleridge Rich Context Competition.</p>
<h1 id="introduction-5">Introduction</h1>
<p>Automated discovery from scientific research publications is an important task for analysts, researchers, and learners as they develop the scientific knowledge and use them to gain new insights. More specifically, on the tasks of discovering datasets and methods mentioned in a research publication, we have seen a lack of available tools to easily find who else worked on a particular dataset, what research methods people apply on the dataset, and what results they have found using the dataset. Furthermore, new datasets are not easy to discover, and as a result, good datasets and methods are often neglected.</p>
<p>The Coleridge Rich Context Competition (RCC) aims to build automated datasets discovery from social science research publications, filling the gap of this problem. In this competition, given a corpus of social science research publications, we have to automatically identify datasets used, and then infer the research methods and research fields in the publications. Note that no labeled data are given for research methods and fields identification.</p>
<p>This manuscript describes summary of our submission for the first phase of RCC. We begin with related work in section [sec:relatedwork]. We present our analysis on RCC dataset in section [sec:data], describe our approach in section [sec:methods], and discuss our experiment results in section [sec:experiments]. Finally, we wrap up with conclusion and future work in section [sec:conclusion].</p>
<h1 id="sec:relatedwork">Related Work</h1>
<p>Extracting information from scientific text has been explored in the past <span class="citation" data-cites="Peng2004AccurateIE Nguyen2015ScholarlyDI Singh2016OCRAR">[@Peng2004AccurateIE; @Nguyen2015ScholarlyDI; @Singh2016OCRAR]</span>. One type of information extraction from scientific articles is extracting keyphrases and relation between them <span class="citation" data-cites="Augenstein2017SemEval2T">[@Augenstein2017SemEval2T]</span>. <span class="citation" data-cites="Luan2017ScientificIE">@Luan2017ScientificIE</span> propose semi-supervised sequence tagging approach to extract keyphrases. <span class="citation" data-cites="Augenstein2017MultiTaskLO">@Augenstein2017MultiTaskLO</span> explore multi-task deep recurrent neural network approach with several auxiliary tasks to extract keyphrases.</p>
<p>Another type of extraction is citation extraction. Two citation extraction settings have been explored before: reference mining inside the full text <span class="citation" data-cites="Alves2018DeepRM">[@Alves2018DeepRM]</span>, and citation metadata extraction <span class="citation" data-cites="Hetzner2008ASM Anzaroot2014LearningSL An2017CitationME">[@Hetzner2008ASM; @Anzaroot2014LearningSL; @An2017CitationME]</span>. <span class="citation" data-cites="Nasar2018InformationEF">@Nasar2018InformationEF</span> write a survey on information extraction from scientific articles.</p>
<p>Recently, there are some work to explore dataset extraction from scientific text <span class="citation" data-cites="Boland2012IdentifyingRT Ghavimi2016ASA Ghavimi2016IdentifyingAI">[@Boland2012IdentifyingRT; @Ghavimi2016ASA; @Ghavimi2016IdentifyingAI]</span>. <span class="citation" data-cites="Boland2012IdentifyingRT">@Boland2012IdentifyingRT</span> propose weakly supervised pattern induction to identify references in social science publications. <span class="citation" data-cites="Ghavimi2016ASA Ghavimi2016IdentifyingAI">@Ghavimi2016ASA [@Ghavimi2016IdentifyingAI]</span> propose a semi automatic approach for detecting dataset references for social science texts. Dataset extraction is a challenging task because of the inconsistency and wide range of dataset mention styles in research publications <span class="citation" data-cites="Ghavimi2016IdentifyingAI">[@Ghavimi2016IdentifyingAI]</span>.</p>
<h1 id="sec:data">Data Analysis</h1>
<p>The first phase of RCC dataset consists of a labeled corpus of 5,000 publications for training set, and additional 100 publications for development set. The RCC organizer keeps a separate corpus of 5,000 publications for evaluation. Each article in the dataset contains full text article and dataset citation labels. The metadata of cited datasets in the corpus are also provided. For research methods and fields, no label information is provided, only SAGE social science research method graph and research fields vocabulary are provided.</p>
<p><strong>Preprocessing.</strong> In order to reliably access important structures of paper publications, we parse all papers using AllenAI Science Parse<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> <span class="citation" data-cites="Ammar2018ConstructionOT">[@Ammar2018ConstructionOT]</span>. AllenAI Science Parse reads PDF file, and returns title, authors, abstract, sections, and bibliography (references). Since this parser utilizes machine learning models to parse PDF file, the parsing results may not be 100% accurate. Furthermore, this parser is unable to parse scan copy of old publication. In the situation where we are unable to access parsed fields, we fall back to the given text files.</p>
<p><strong>Mention Analysis.</strong> There are 5,499 and 123 dataset citations in training and development set respectively. Among these citations, 320 citations in training set and 6 citations in development set do not have mentions information. We analyze the paper sections where the dataset mentions commonly occur. Table [tab:train_top_sections] and [tab:dev_top_sections] show top 12 most common sections mentioning dataset in training and development set. The tables suggest that abstract, reference titles, discussion, results, and methods are the most common sections where the dataset mentions occur. We exploit reference titles for dataset extraction.</p>
<table>
<caption>[tab:train_top_sections] Top 12 Sections Mentioning Datasets in Training Set</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Section Header</th>
<th style="text-align: right;">Mention Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Abstract</td>
<td style="text-align: right;">2,548</td>
</tr>
<tr class="even">
<td style="text-align: left;">Reference Titles</td>
<td style="text-align: right;">1,997</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Discussion</td>
<td style="text-align: right;">1,390</td>
</tr>
<tr class="even">
<td style="text-align: left;">Results</td>
<td style="text-align: right;">836</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Methods</td>
<td style="text-align: right;">804</td>
</tr>
<tr class="even">
<td style="text-align: left;">Introduction</td>
<td style="text-align: right;">530</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Statistical Analysis</td>
<td style="text-align: right;">285</td>
</tr>
<tr class="even">
<td style="text-align: left;">Comment</td>
<td style="text-align: right;">279</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Acknowledgements</td>
<td style="text-align: right;">261</td>
</tr>
<tr class="even">
<td style="text-align: left;">Materials and Methods</td>
<td style="text-align: right;">254</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Study Population</td>
<td style="text-align: right;">227</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data</td>
<td style="text-align: right;">214</td>
</tr>
</tbody>
</table>
<table>
<caption>[tab:dev_top_sections] Top 12 Sections Mentioning Datasets in Development Set</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Section Header</th>
<th style="text-align: right;">Mention Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Abstract</td>
<td style="text-align: right;">78</td>
</tr>
<tr class="even">
<td style="text-align: left;">Reference Titles</td>
<td style="text-align: right;">37</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Discussion</td>
<td style="text-align: right;">19</td>
</tr>
<tr class="even">
<td style="text-align: left;">Introduction</td>
<td style="text-align: right;">14</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Results</td>
<td style="text-align: right;">12</td>
</tr>
<tr class="even">
<td style="text-align: left;">Statistical Analyses</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Methods</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ethics</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Population</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">Population Impact</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Price</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">2.1 Data</td>
<td style="text-align: right;">5</td>
</tr>
</tbody>
</table>
<p><strong>Citation Analysis.</strong> We build citation network from training set. Each node in the network is a paper publication, and an edge between two node <span class="math inline"><em>A</em></span> and <span class="math inline"><em>B</em></span> is generated if a paper <span class="math inline"><em>A</em></span> cites paper <span class="math inline"><em>B</em></span>. Table [tab:network_stats] shows the statistics of the citation network.</p>
<table>
<caption>[tab:network_stats] Statistics of Citation Network</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">Number of nodes</td>
<td style="text-align: right;">5,000</td>
</tr>
<tr class="even">
<td style="text-align: left;">Number of edges</td>
<td style="text-align: right;">998</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Network density</td>
<td style="text-align: right;">0.008%</td>
</tr>
</tbody>
</table>
<p>Initially, we propose an approach utilizing citation network based on an intuition that datasets, research methods, and research fields are shared by: 1) same or similar issues, 2) same or similar context, 3) same or similar authors and communities, 4) same or similar metrics used in the publication. However, based on table [tab:network_stats], we learn that exploring rich context using paper-paper citation network is not viable at this stage because most papers listed in publications’ bibliography are not available in the training set, and therefore, paper-paper citation network becomes very sparse with many unknown information. Due to this reason, we drop our idea on utilizing paper-paper citation graph at this stage. Nevertheless, we believe that bibliography contains important signals and information about datasets, and research fields.</p>
<h1 id="sec:methods">Methods</h1>
<p>In this section, we describe our approach for RCC tasks: dataset extraction, research methods identification, and research fields identification.</p>
<h2 id="ssec:dataset_extraction">Dataset Extraction</h2>
<p>We employ a pipeline of two subtasks for dataset extraction: dataset detection, followed by dataset recognition. The goal of dataset detection is to detect whether a publication cites a dataset or not. This first subtask helps us to quickly filter out non-dataset publications. After the first subtask, we mine dataset mentions for the remaining publications in dataset recognition subtask.</p>
<p>For dataset detection, we utilize paper title in bibliography (reference list) combined with explicit research methods mentions to detect whether a publication citing a dataset or not. Explicit research methods mentions are determined based on exact match between paper title and SAGE research methods vocabulary. We train an SVM classifier using explicit research method mentions and n-gram features from paper titles in bibliography. We use the SVM classifier to classify each publication, if the classifier gives positive label, then we proceed to dataset recognition subtask, otherwise we ignore the publication.</p>
<p>For dataset recognition, we use an implicit entity linking approach. We start with the Naive Bayes model, which can be regarded as a standard information retrieval baseline, and entity indicative weighting strategy is used to improve the model. In order to calculate the word distribution of each dataset, we represent each dataset using its title, dataset mentions (provided in the training set), and dataset relevant sentences, filtered from the relevant publications using the rule based approach proposed in <span class="citation" data-cites="Ghavimi2016IdentifyingAI">@Ghavimi2016IdentifyingAI</span>. All these text sections related to a particular dataset are considered as a single text chunk, and we calculate the word distribution as follows. Let <span class="math inline"><strong>w</strong></span> be the set of words in a dataset. In our problem setting, we assume the dataset prior probability <span class="math inline"><em>p</em>(<em>d</em>)</span> to be uniform. The probability of dataset <span class="math inline"><em>d</em></span> given <span class="math inline"><em>w</em> ∈ <strong>w</strong></span> is:</p>
<p><br /><span class="math display">$$\begin{split}
    p(d|\textbf{w}) &amp; \propto \prod _{w \in \textbf{w}} p(w|d) \\
    &amp; = \prod _{w \in \textbf{w}} \frac{f(d,w) + \gamma }{ \sum_{w'} f(d,w') + |W| \gamma}
\end{split}$$</span><br /></p>
<p>where <span class="math inline"><em>f</em>(<em>d</em>, <em>w</em>)</span> is the number of co-occurrences of word <span class="math inline"><em>w</em></span> with entity <span class="math inline"><em>d</em></span>, <span class="math inline"><em>γ</em></span> is the smoothing parameter, and <span class="math inline">|<em>W</em>|</span> is the vocabulary size. For each dataset <span class="math inline"><em>d</em></span>, we derive <span class="math inline"><em>f</em>(<em>d</em>, <em>w</em>)</span> by the count of <span class="math inline"><em>w</em></span> occurrences in the text extracted for each dataset. In order to stress more priority for dataset indicative words, we improved the final objective function of our model as follows:</p>
<p><br /><span class="math display"><em>l</em><em>n</em>(<em>p</em>(<em>d</em>|<strong>w</strong>)) ∝ ∑<sub><em>w</em> ∈ <strong>w</strong></sub><em>β</em>(<em>w</em>) * <em>l</em><em>n</em>(<em>p</em>(<em>w</em>|<em>d</em>))</span><br /></p>
<p>where <span class="math inline"><em>β</em>(<em>w</em>)</span> is the entity-indicative weight for word <span class="math inline"><em>w</em></span>. This weight <span class="math inline"><em>β</em>(<em>w</em>)</span> is added as an exponent to the term <span class="math inline"><em>p</em>(<em>w</em>|<em>d</em>)</span>. <span class="math inline"><em>β</em>(<em>w</em>)</span> is calculated as:</p>
<p><br /><span class="math display"><em>β</em>(<em>w</em>) = <em>l</em><em>o</em><em>g</em>(1 + <em>E</em>/<em>d</em><em>f</em>(<em>w</em>))</span><br /></p>
<p>where <span class="math inline"><em>E</em></span> is the number of distinct datasets considered and <span class="math inline"><em>d</em><em>f</em>(<em>w</em>)</span> counts the number of datasets with at least one occurrence of w.</p>
<p>Then for a given unseen publication, we use same rule based approach <span class="citation" data-cites="Ghavimi2016IdentifyingAI">[@Ghavimi2016IdentifyingAI]</span> to filter a few relevant sentences, and datasets are ranked by <span class="math inline"><em>l</em><em>n</em>(<em>p</em>(<em>d</em>|<em>w</em>))</span> to select the most suitable datasets. In order to select exact datasets related to particular publication, we select top 10 datasets ranked using above approach. And then the confidence probability related to the top 10 datasets are normalized and select the datasets with the normalized probability higher than a predefined threshold value. We return the entity indicative words as relevant dataset mentions.</p>
<h2 id="ssec:research_method_identification">Research Methods Identification</h2>
<p>Since we do not have labeled training data for this task, we use explicit research method mentions (based on exact match with SAGE research methods vocabulary) in a publication as weak signals on research methods used in the publication. When these mentions frequently appear in a publication, there is a high chance that this publication is using these particular research methods.</p>
<p>Based on this intuition, we generate training set for research method classification utilizing sentences that explicitly mention research method in a publication. Publication title and the sentences mentioning research method serve as context information of a specific research method. In order to reduce noisy weak signals, we apply minimum support of three sentences in a publication. We exclude research methods which only being mentioned one or two times in a publication. We also exclude research methods that only being mentioned in less than 10 different publications from the training set. Finally, we have 133 research methods having sufficient context information for training data. This number is 20.18% of 659 research methods in SAGE research method graph.</p>
<p>We use the training data to train logistic regression classifier to classify research methods from publication title and sentences. We utilize n-gram features from publication title and sentences for the classifier. We apply the logistic regression classifier to recommend top 3 research methods based on logistic regression probability score.</p>
<p>This approach can be extended by utilizing research method graph to expand the context. Context information does not only comes from sentences in publication, but also comes from related research methods as well as broader concept information. By using this information, we can potentially expand to more than 133 research methods and perform more accurate prediction.</p>
<h2 id="ssec:research_field_identification">Research Fields Identification</h2>
<p>Similar to research methods identification, this task does not have labeled training data. We only have access to list of SAGE research fields. SAGE research fields are organized hierarchically into three levels, namely L1, L2, and L3, for example: Soc-2-4 (<em>kinship</em>) is under Soc (<em>sociology</em>) in L1, and under Soc-2 (<em>anthropology</em>) in L2.</p>
<p>To gain more understanding about the characteristic of each field, we crawl top search results from SAGE Knowledge<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. From the search result snippets, we collect information such as title and abstract on various publications including case, major work, books, handbooks, and dictionary. We exclude video and encyclopedia. Due to sparseness of the SAGE Knowledge, we exclude all research fields with less than 10 search results. In the end, we have samples of 414 L3 research fields under 101 L2 research fields and 10 L1 research fields. This numbers cover 20.87% of 1,984 L3 research fields, and 67.79% of 149 L2 research fields in the list of SAGE research fields. We use this data to train research fields classifiers.</p>
<p>We build three SVM classifiers for L1, L2, and L3 to classify a publication using paper title and abstract. Instead of taking the highest score, we take top-k research fields and perform re-ranking considering agreement among L1, L2, L3. We return a research field if its upper level are also in top ranks. Since level L1 is too general, we only output research fields from L2, and L3. We outline our heuristic to reorder the ranking below:</p>
<ol type="1">
<li><p>Get top-5 L3 research fields, top-4 L2 research fields, and top-3 L1 research fields.</p></li>
<li><p>Assign initial score <span class="math inline"><em>v</em></span> for each research field based on its ranking. <br /><span class="math display"><em>v</em>(<em>f</em><sub><em>i</em></sub>) = (<em>K</em> − <em>i</em>)/<em>K</em></span><br /> where <span class="math inline"><em>K</em></span> is the length of top-k, and <span class="math inline"><em>i</em></span> is the ranking of a research field <span class="math inline"><em>f</em></span>. For example, research fields in top-5 L3 have initial score of <span class="math inline">[1, 0.8, 0.6, 0.4, 0.2]</span>, top-4 L2 have initial score of <span class="math inline">[1, 0.75, 0.5, 0.25]</span>, and top-3 L1 have <span class="math inline">[1, 0.666, 0.333]</span></p></li>
<li><p>Update the score by multiplying each score with the score of matching research fields at upper level, and <span class="math inline">0</span> otherwise. <br /><span class="math display">$$score(f_i^l) =
        \begin{cases}
        \prod _{l \in L} v(f^l) &amp; \text{if field matched} \\
        0 &amp; \text{otherwise}
        \end{cases}$$</span><br /> where L is the level of research field <span class="math inline"><em>f</em></span> and its upper levels. Here are examples of score update:</p>
<ul>
<li><p>Soc-2-4 at rank-2 in L3, Soc-2 at rank-3 in L2, and Soc at rank-1 in L1. In this case, the score of Soc-2-4 is <span class="math inline">0.8 * 0.5 * 1 = 0.4</span>.</p></li>
<li><p>Soc-2-4 at rank-1 in L3, Soc-2 at rank-2 in L2, but Soc is not found in top rank in L1. In this case, the score of Soc-2-4 is <span class="math inline">0</span>.</p></li>
</ul></li>
<li><p>Collect score from L2 and L3, and exclude L2 if we see more specific of L2 in top-5 L3.</p></li>
<li><p>Re-rank L2 and L3 research fields based on the score.</p></li>
<li><p>Return research fields having score <span class="math inline"> &gt;  = 0.4</span>.</p></li>
</ol>
<p>To expand to more context from paper list in bibliography section, we also build other three Naive Bayes classifiers for L1, L2, and L3 using paper title feature only. We believe that a publication from a certain field also cites other publications from same or similar fields. For each publication in the bibliography, we apply the same procedure as mentioned above, then we average the score to get top research fields from bibliography. Finally, we combine top research fields from paper titles and abstract with results from bibliography.</p>
<h1 id="sec:experiments">Experiment Results</h1>
<p>We discuss our experiment results for each task in this section. We use standard precision, recall, and F1 as evaluation metrics.</p>
<p><strong>Dataset Extraction.</strong> First, we analyze our experiment for dataset detection subtask comparing Naive Bayes and SVM classifier. Using only paper titles in bibliography and explicit research method mentions, Naive Bayes and SVM classifiers are able to reach 0.88 &amp; 0.92 F1 score respectively. Since SVM outperforms Naive Bayes, we use SVM for our dataset detection module. Table [tab:dd_dev_result] shows detail dataset detection results on development set.</p>
<table>
<caption>[tab:dd_dev_result] Dataset Detection Results on Development Set</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Classifier &amp; </strong>Prec. &amp; <strong>Rec. &amp; </strong>F1\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">Naive Bayes &amp; 0.85 &amp; 0.92 &amp; 0.88\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">SVM &amp; 0.96 &amp; 0.88 &amp; 0.92\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">********</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>To see the impact of performing dataset detection, we test the performance of dataset extraction with and without dataset detection on development set. Table [tab:de_dev_result] summarizes the results. As shown in the table, performing dataset detection before extraction significantly improves the dataset extraction on development set.</p>
<table>
<caption>[tab:de_dev_result] Dataset Extraction Results on Development Set</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Method &amp; </strong>Prec. &amp; <strong>Rec. &amp; </strong>F1\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">No Dataset Detection &amp; 0.18 &amp; 0.33 &amp; 0.24\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">With Dataset Detection &amp; 0.34 &amp; 0.30 &amp; 0.32\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">********</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<caption>[tab:de_test_result] Dataset Extraction Result on Test Set</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Dataset &amp; </strong>Prec. &amp; <strong>Rec. &amp; </strong>F1\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">Test Set (phase1) &amp; 0.17 &amp; 0.10 &amp; 0.13\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">********</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table [tab:de_test_result] shows dataset extraction performance on test set (phase 1). The significant drop from development set result suggests that the test set might have different distribution compare to the training and development set. It might also contain dataset citations that are never been seen in training set.</p>
<p><strong>Research Methods Identification.</strong> We only consider Naive Bayes and Logistic Regression classifiers for research method identification because they naturally outputs probability score. We perform 5-fold cross validation to evaluate classification performance, and the result can be seen in table [tab:rmethods_5cv]. Logistic regression classifier outperforms Naive Bayes with 0.86 F1 score in classifying 133 research methods.</p>
<table>
<caption>[tab:rmethods_5cv] F1 Score for Research Method Classification</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Classifier &amp; </strong>F1\</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">Naive Bayes &amp; 0.55\</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Logistic Regression &amp; 0.86\</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">****</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Research Fields Identification.</strong> We perform 5-fold cross validation to evaluate our classifiers to classify L1, L2, and L3 research fields. Table [tab:rfields_pub_5cv] shows the results using n-gram features from paper title and abstract, whereas table [tab:rfields_rt_5cv] shows the results using n-gram features from title only. Naive Bayes tends to perform slightly better on L3 research fields where we have large number of research field labels. We decide to use SVM for research field identification on publication level because SVM is generally better than Naive Bayes. On the other hand, we decide to use Naive Bayes for research field identification on bibliography level because Naive Bayes prefer to have more accurate L2 and L3 research fields.</p>
<table>
<caption>[tab:rfields_pub_5cv] F1 Score for Research Field Classification on Publication Level using Paper Title and Abstract</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Classifier &amp; </strong>L1 &amp; <strong>L2 &amp; </strong>L3\</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">Naive Bayes &amp; 0.78 &amp; 0.37 &amp; 0.13\</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">SVM &amp; 0.82 &amp; 0.38 &amp; 0.12\</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">********</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<caption>[tab:rfields_rt_5cv] F1 Score for Research Field Classification on Bibliography Level using Paper Title Only</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Classifier &amp; </strong>L1 &amp; <strong>L2 &amp; </strong>L3\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">Naive Bayes &amp; 0.80 &amp; 0.35 &amp; 0.12\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">SVM &amp; 0.81 &amp; 0.35 &amp; 0.11\</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">********</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="sec:conclusion">Conclusion</h1>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Method &amp; </strong>Features (n-gram)\</td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">\</td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">SVM for dataset detection &amp; paper titles in bibliography and explicit research method mentions\</td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">Implicit entity linking &amp; paper title and full text\</td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">\</td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">Logistic regression &amp; paper title, abstract, and full text\</td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">\</td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">SVM (on paper) &amp; paper title and abstract\</td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Naive Bayes (on bibliography) &amp; paper titles in bibliography\</td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">****</td>
<td></td>
</tr>
</tbody>
</table>
<p>Extraction of research datasets, associated research methods and fields from social science publication is challenging, yet an important problem to organize social science publications. We have described our approach for the RCC challenge, and table [tab:summary] summarizes our approach. Beside publication content such as paper titles, abstract, full text, our approach also leverages on the information from bibliography. Furthermore, we also collect external information from SAGE Knowledge to get more information about research fields.</p>
<p>Apart from F1 score on 5-fold cross validation, we have no good way to evaluate research method and research field identification without ground truth label. Our methods are unable to automatically extract and recognize new datasets, research methods, and fields. An extension to automatically handle such cases using advance Natural Language Processing (NLP) approach is a promising direction.</p>
<p>From this competition, we have learned that lacks of labelled training data is a huge challenge, and it directs us to other external resources (i.e., SAGE Knowledge) as proxy for our label. Another challenge is data sparsity. Although we see many paper listed in bibliography, lacks of access to these publication make us difficult to exploit citation network.</p>
<p>Unfortunately, our model did not advance to the second phase. We are interested in exploring more advanced information extraction methods on the RCC datasets, and we hope that the organizer will release the RCC datasets for future research. We thank the organizers for organizing a competition and workshop on this important, interesting, and challenging problem.</p>
<h2 id="placeholder-for-reseach-agenda-and-next-steps-chapter.">Placeholder for Reseach Agenda and Next Steps chapter.</h2>
<p>abstract: | Datasets are critical for scientific research, playing a role in replication, reproducibility, and efficiency. Researchers have recently shown that datasets are becoming more important for science to function properly, even serving as artifacts of study themselves. However, citing datasets is not a common or standard practice in spite of recent efforts by data repositories and funding agencies. This greatly affects our ability to track their usage and importance. A potential solution to this problem is to automatically extract dataset mentions from scientific articles. In this work, we propose to achieve such extraction by using a neural network based on a BiLSTM-CRF architecture. Our method achieves <span class="math inline"><em>F</em><sub>1</sub> = 0.883</span> in social science articles released as part of the Rich Context Dataset. We discuss limitations of the current datasets and propose modifications to the model to be done in the future. author: - ‘Tong Zeng<span class="math inline"><em></em><sup>1, 2</sup></span> and Daniel Acuna<span class="math inline"><em></em><sup>1</sup></span><a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>’ bibliography: - ‘rcc-06.bib’ title: | Dataset mention extraction in scientific articles using a BiLSTM-CRF model —</p>
<h1 id="introduction-6">Introduction</h1>
<p>Science is fundamentally an incremental discipline that depends on previous scientist’ work. Datasets form an integral part of this process and therefore should be shared and cited as any other scientific output. This ideal is far from reality; the credit that datasets currently receive does not correspond to their actual usage. One of the issues is that there is no standard approach for citing them. Interestingly, while datasets are still used and mentioned in articles, we lack methods to extract such mentions and properly reconstruct dataset citations. The Rich Context Competition challenge aims at closing this gap by inviting scientists to produce automated dataset mention and linkage detection algorithms. In this article, we detail our proposal to solve the dataset mention step. Our approach attempts to provide a first approximation to better give credit and keep track of datasets and their usage.</p>
<p>The problem of dataset extraction has been explored before. <span class="citation" data-cites="ghavimiIdentifyingImprovingDataset2016">@ghavimiIdentifyingImprovingDataset2016</span> and <span class="citation" data-cites="ghavimiSemiautomaticApproachDetecting2017">@ghavimiSemiautomaticApproachDetecting2017</span> use a relatively simple tf-idf representation with cosine similarity for matching dataset identification in social science articles. Their method consists of four major steps: preparing a curated dictionary of typical mention phrases, detecting dataset references, and ranking matching datasets based on cosine similarity of tf-idf representations. This approach achieved an impressive <span class="math inline"><em>F</em><sub>1</sub> = 0.84</span> for mention detection and <span class="math inline"><em>F</em><sub>1</sub> = 0.83</span>, for matching. <span class="citation" data-cites="singhalDataExtractMining2013">@singhalDataExtractMining2013</span> proposed a method using normalized Google distance to screen whether a term is in a dataset. However, this method relies on external services and is not computational efficient. They achieve a good <span class="math inline"><em>F</em><sub>1</sub> = 0.85</span> using Google search and <span class="math inline"><em>F</em><sub>1</sub> = 0.75</span> using Bing. A somewhat similar project was proposed by <span class="citation" data-cites="luDatasetSearchEngine2012">@luDatasetSearchEngine2012</span>. They built a dataset search engine by solving the two challenges: identification of the dataset and association to a URL. They build a dataset of 1000 documents with their URLs, containing 8922 words or abbreviations representing datasets. They also build a web-based interface. This shows the importance of dataset mention extraction and how several groups have tried to tackle the problem.</p>
<p>In this article, we describe a method for extracting dataset mentions based on a deep recurrent neural network. In particular, we used a Bidirectional Long short-term Memory (BiLSTM) sequence to sequence model paired with a Conditional Random Field (CRF) inference mechanism. We tested our model on a novel dataset produced for the Rich Context Competition challenge. We achieve a relatively good performance of <span class="math inline"><em>F</em><sub>1</sub> = 0.883</span>. We discuss the current noise and duplication present in the dataset and limitations of our model.</p>
<h1 id="the-dataset">The dataset</h1>
<p>The Rich Context Dataset challenge was proposed by the New York University’s Coleridge Initiative <span class="citation" data-cites="richtextcompetition">[@richtextcompetition]</span>. The challenge comprised several phases, and participants moved through the phases depending on their performance. We only analyze data of the first phase. This phase contained a list of datasets and a labeled corpus of around 5K publications. Each publication was labeled indicating whether a dataset was mentioned within it and which part of the text mentioned it. The challenge used the accuracy for measuring the performance of the competitors and also the quality of the code, documentation, and efficiency.</p>
<p>We adopt the CoNLL 2003 format <span class="citation" data-cites="tjong2003introduction">[@tjong2003introduction]</span> to annotate whether a token is a part of dataset mention. Concretely, we use we use B-DS denotes a token is the first token of a dataset mention, I-DS denote a token is inside of dataset mention, and O means a token is not a part of dataset mention. We then put each token and its corresponding labels in one line and use a empty line as separator between sentences. All the sentences was split by 70%, 15%, 15% as training set, validation set and testing set.</p>
<h1 id="the-proposed-method">The Proposed Method</h1>
<h2 id="overall-view-of-the-architecture">Overall view of the architecture</h2>
<p>In this section, we propose a model for detecting mentions based on a BiLSTM-CRF architecture. At a high level, the model uses a sequence-to-sequence recurrent neural network that produces the probability of whether a token belongs to a dataset mention. The CRF layer takes those probabilities and estimates the most likely sequence based on constrains between label transitions (i.e., mention–to–no-mention–to-mention has low probability). While this is a standard architecture for modeling sequence labeling, the application to our particular dataset and problem is new.</p>
<p>We now describe in more detail the choices of word representation, hyper-parameters, and training parameters. A schematic view of the model is in Fig [fig:NetworkArchitecture] and the components are as follows:</p>
<ol type="1">
<li><p>Input Layer: input the sequence of tokens to the network;</p></li>
<li><p>Embedding layer: mapping each token into fixed sized vector representation based on fasttext (200-dimensional vectors,</p>
<ol class="example" type="1">
<li></li>
</ol></li>
<li><p>One BiLSTM layer: make use of Bidirectional LSTM network to capture the high level representation of the whole token sequence input (200 dimensions per direction, totally 400 output units)</p></li>
<li><p>Dense layer: project the output of the previous layer to a low dimensional vector representation of the the distribution of labels.</p></li>
<li><p>CRF layer: find the most likely sequence of labels.</p></li>
</ol>
<figure>
<embed src="img/bilstm-crf-model.pdf" style="width:5cm" /><figcaption>[fig:NetworkArchitecture]Network Architecture of BiLSTM-CRF model</figcaption>
</figure>
<h2 id="word-embedding">Word Embedding</h2>
<p>The embedding is the first layer of our network and it is responsible for mapping the word from string into vectors of numbers as the input for other layers on top. For a given sentence <span class="math inline"><em>S</em></span>, we first convert it into a sequence consisting of <span class="math inline"><em>n</em></span> tokens, <span class="math inline"><em>S</em> = {<em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, ⋯, <em>c</em><sub><em>n</em></sub>, }</span> . For each token <span class="math inline"><em>c</em><sub><em>i</em></sub></span>we lookup the embedding vector <span class="math inline"><em>x</em><sub><em>i</em></sub></span> from a word embedding matrix <span class="math inline"><em>M</em><sup><em>t</em><em>k</em><em>n</em></sup> ∈ ℝ<sup><em>d</em>|<em>V</em>|</sup></span>, where the <span class="math inline"><em>d</em></span> is the dimension of the embedding vector and the <span class="math inline"><em>V</em></span> is the Vocabulary of the tokens. In this paper, the matrix <span class="math inline"><em>M</em><sup><em>t</em><em>k</em><em>n</em></sup></span> is initialized by a pre-trained embedding, but will be updated by learning from our corpus.</p>
<h2 id="lstm">LSTM</h2>
<p>Recurrent neural network (RNN) is a powerful tool to capture features from sequential data, such as temporal series, and text. RNN could capture long-distance dependency in theory but it suffers from the gradient exploding/vanishing problems <span class="citation" data-cites="pascanu2013difficulty">[@pascanu2013difficulty]</span>. The Long short-term memory (LSTM) architecture was proposed by <span class="citation" data-cites="hochreiter1997long">@hochreiter1997long</span> and it is a variant of RNN which copes with the gradient problem. LSTM introduces several gates to control the proportion of information to forget from previous time steps and to pass to the next time step. Formally, LSTM could be described by the following equations:</p>
<p><br /><span class="math display"><em>i</em><sub><em>t</em></sub> = <em>σ</em>(<em>W</em><sub><em>i</em></sub><em>x</em><sub><em>t</em></sub> + <em>W</em><sub><em>i</em></sub><em>h</em><sub><em>t</em> − 1</sub> + <em>b</em><sub><em>i</em></sub>)</span><br /></p>
<p><br /><span class="math display"><em>f</em><sub><em>t</em></sub> = <em>σ</em>(<em>W</em><sub><em>f</em></sub><em>x</em><sub><em>t</em></sub> + <em>W</em><sub><em>f</em></sub><em>h</em><sub><em>t</em> − 1</sub> + <em>b</em><sub><em>f</em></sub>)</span><br /></p>
<p><br /><span class="math display"><em>g</em><sub><em>t</em></sub> = <em>t</em><em>a</em><em>n</em><em>h</em>(<em>W</em><sub><em>g</em></sub><em>x</em><sub><em>t</em></sub> + <em>W</em><sub><em>g</em></sub><em>h</em><sub><em>t</em> − 1</sub> + <em>b</em><sub><em>g</em></sub>)</span><br /></p>
<p><br /><span class="math display"><em>o</em><sub><em>t</em></sub> = <em>σ</em>(<em>W</em><sub><em>o</em></sub><em>x</em><sub><em>t</em></sub> + <em>W</em><sub><em>o</em></sub><em>h</em><sub><em>t</em> − 1</sub> + <em>b</em><sub><em>o</em></sub>)</span><br /></p>
<p><br /><span class="math display"><em>c</em><sub><em>t</em></sub> = <em>f</em><sub><em>t</em></sub>⨂<em>c</em><sub><em>t</em> − 1</sub> + <em>i</em><sub><em>t</em></sub>⨂<em>g</em><sub><em>t</em></sub></span><br /></p>
<p><br /><span class="math display"><em>h</em><sub><em>t</em></sub> = <em>o</em><sub><em>t</em></sub>⨂<em>t</em><em>a</em><em>n</em><em>h</em>(<em>c</em><sub><em>t</em></sub>)</span><br /></p>
<p>where the <span class="math inline"><em>σ</em></span> is the sigmoid function, <span class="math inline">⨂</span> denotes the dot product, <span class="math inline"><em>b</em></span> is the bias, <span class="math inline"><em>W</em></span> is the parameters, <span class="math inline"><em>x</em><sub><em>t</em></sub></span> is the input at time <span class="math inline"><em>t</em></span>, <span class="math inline"><em>c</em><sub><em>t</em></sub></span> is the LSTM cell state at time <span class="math inline"><em>t</em></span> and <span class="math inline"><em>h</em><sub><em>t</em></sub></span> is hidden state at time <span class="math inline"><em>t</em></span>. The <span class="math inline"><em>i</em><sub><em>t</em></sub></span>, <span class="math inline"><em>f</em><sub><em>t</em></sub></span>, <span class="math inline"><em>o</em><sub><em>t</em></sub></span> and <span class="math inline"><em>g</em><sub><em>t</em></sub></span> are named as input, forget, output and cell gates respectively, they control the informations to keep in its state and pass to next step.</p>
<p>LSTM get information from the previous steps, that is left context in our task. However, it is important to consider the information in the right context. A solution of this information need is bidirectional LSTM <span class="citation" data-cites="graves2013speech">[@graves2013speech]</span>. The idea of Bi-LSTM is using two LSTM layers and feed in each layer with sequence forwards and backwards separately, and then concatenate the hidden states of the two LSTM to modeling both the left and right contexts</p>
<p><br /><span class="math display">$$h_{t}=[\overrightarrow{h_{t}}\varoplus\overleftarrow{h_{t}}]$$</span><br /></p>
<p>Finally, the outcomes of the states are taken by a Conditional Random Field (CRF) layer that takes into account the transition nature of the beginning, intermediate, and ends of mentions. For a reference of CRF, refer to <span class="citation" data-cites="lafferty2001conditional">[@lafferty2001conditional]</span></p>
<h1 id="results">Results</h1>
<p>In this work, we wanted to propose a model for the Rich Context Competition challenge. We propose a relatively standard architecture based on a BiLSTM-CRF recurrent neural network. We now describe the results of this network on the dataset provided by the competition.</p>
<p>For all of our results, we use <span class="math inline"><em>F</em><sub>1</sub></span> as the measure of choice. This measure is the harmonic average of the precision and recall and it is the standard measure used in sequence labeling tasks. This metric varies from 0 to 1, and the unit is the highest possible value. Our method achieved a relatively high <span class="math inline"><em>F</em><sub>1</sub></span> of 0.883 for detecting mentions, in line with previous studies.</p>
<p>We found significant limitations to the dataset, and we expect these limitations to affect the linkage step (not done in this article). While we are proposing a model for such step, we found that it would be challenging to do so given the quality of the annotations. Specifically, we found significant duplication of labels. The first issue is that mentions are nested (e.g. HRS, RAND HRS, HRS DATA, RAND HRS DATA are nested and linked to the same dataset). The second issue is that for the same mention text, several, different datasets were linked (e.g. the term CPS is linked to 57 datasets, the term NHANES is linked to 32 datasets). This adds noise to the linkage process. In fact, most of the mentions have ambiguous relationships to datasets. In particular, only 17,267 (16.99%) mentions are linked to one dataset, 15,292 (15.04%) mentions are listed to two datasets, and 12,624 (12.42%) are linked to three datasets. We found that there were some extreme cases, where for example there are several mentions linked to more than one hundred datasets. If these difficulties are not overcome, then the predictions from the linkage process will be noisy and therefore impossible to tell apart.</p>
<h1 id="conclusion-1">Conclusion</h1>
<p>In this work, we report a high accuracy model for the problem of detecting dataset mentions. Because our method is based on a standard BiLSTM-CRF architecture, we expect that updating our model with recent developments in neural networks would only benefit our results. We also provide some evidence of how difficult we believe the linkage step of the challenge could be if the dataset noise are not lowered.</p>
<p>One of the shortcomings of our approach is that the architecture is lacking some modern features of RNN networks. In particular, recent work has shown that attention mechanisms are important especially when the task requires spatially distant information, such as this one. These benefits could also translate to better linkage. We are exploring new architectures using self-attention and multiple-head attention. We hope to explore these approaches in the near future.</p>
<p>Our proposal, however, is surprisingly effective. Because we have barely modified a general RNN architecture, we expect that our results will generalize relatively well either to the second phase of the challenge or even to other disciplines. We would emphasize, however, that the quality of the dataset has a great deal of room for improvement. Given how important this task is for the whole of science, we should try to strive to improve on the quality of these datasets so that techniques like this one can be more broadly applied. The importance of dataset mention and linkage therefore could be fully appreciated by the community.</p>
<h1 id="acknowledgements" class="unnumbered">Acknowledgements</h1>
<p>Tong Zeng was funded by the China Scholarship Council #201706190067. Daniel E. Acuna was funded by the National Science Foundation awards #1646763 and #1800956.</p>
<p>Placeholder for Dimensions use case chapter.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Corresponding author: deacuna@syr.edu<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn2" role="doc-endnote"><p>http://sk.sagepub.com/browse/<a href="#fnref2" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn3" role="doc-endnote"><p><span id="poppler" label="poppler">[poppler]</span><a href="https://manpages.debian.org/testing/poppler-utils" class="uri">https://manpages.debian.org/testing/poppler-utils</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn4" role="doc-endnote"><p>Corresponding author: deacuna@syr.edu<a href="#fnref4" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn5" role="doc-endnote"><p>http://sk.sagepub.com/browse/<a href="#fnref5" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn6" role="doc-endnote"><p><span id="poppler" label="poppler">[poppler]</span><a href="https://manpages.debian.org/testing/poppler-utils" class="uri">https://manpages.debian.org/testing/poppler-utils</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn7" role="doc-endnote"><p><a href="https://github.com/explosion/spaCy" class="uri">https://github.com/explosion/spaCy</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn8" role="doc-endnote"><p><a href="https://wiki.dbpedia.org/services-resources/ontology" class="uri">https://wiki.dbpedia.org/services-resources/ontology</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn9" role="doc-endnote"><p><a href="https://en.wikipedia.org/wiki/Category:Statistical_methods" class="uri">https://en.wikipedia.org/wiki/Category:Statistical_methods</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn10" role="doc-endnote"><p><a href="https://rasa.com/docs/nlu" class="uri">https://rasa.com/docs/nlu</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn11" role="doc-endnote"><p>Corresponding author: deacuna@syr.edu<a href="#fnref11" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn12" role="doc-endnote"><p>http://sk.sagepub.com/browse/<a href="#fnref12" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn13" role="doc-endnote"><p>Corresponding author: deacuna@syr.edu<a href="#fnref13" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>
